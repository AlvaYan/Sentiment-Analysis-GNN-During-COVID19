{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuing-contemporary",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This script preprocess comment and submission combined graph.\n",
    "#Output preprocessed graphs should include input feature, class label, train index, test index, adjacency matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "extra-lindsay",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load 2019 roberta model\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.special import softmax\n",
    "# Preprocess text (username and link placeholders)\n",
    "\n",
    "path_preprocessed=\"C:\\\\Backup of covid project\\\\2cls_CScombined_negVSnonneg\\\\data\\\\\"\n",
    "path_model=\"C:\\\\Backup of covid project\\\\2cls_CScombined_negVSnonneg\\\\\"\n",
    "path_upprocessed=\"C:\\\\Backup of covid project\\\\2cls_CScombined_negVSnonneg\\\\data\\\\\"\n",
    "\n",
    "\n",
    "os.chdir(path_model)\n",
    "\n",
    "def preprocess(text):\n",
    "    new_text = []\n",
    " \n",
    " \n",
    "    for t in text.split(\" \"):\n",
    "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
    "        t = 'http' if t.startswith('http') else t\n",
    "        new_text.append(t)\n",
    "    return \" \".join(new_text)\n",
    "\n",
    "# Tasks:\n",
    "# emoji, emotion, hate, irony, offensive, sentiment\n",
    "# stance/abortion, stance/atheism, stance/climate, stance/feminist, stance/hillary\n",
    "\n",
    "task='sentiment'\n",
    "MODEL = f\"cardiffnlp/twitter-roberta-base-{task}\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "# download label mapping\n",
    "labels=[]\n",
    "mapping_link = f\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/{task}/mapping.txt\"\n",
    "with urllib.request.urlopen(mapping_link) as f:\n",
    "    html = f.read().decode('utf-8').split(\"\\n\")\n",
    "    csvreader = csv.reader(html, delimiter='\\t')\n",
    "labels = [row[1] for row in csvreader if len(row) > 1]\n",
    "\n",
    "# PT\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "model.save_pretrained(MODEL)\n",
    "\n",
    "text = \"Good night ðŸ˜Š\"\n",
    "text = preprocess(text)\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)\n",
    "scores = output[0][0].detach().numpy()\n",
    "scores = softmax(scores)\n",
    "\n",
    "# # TF\n",
    "# model = TFAutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "# model.save_pretrained(MODEL)\n",
    "\n",
    "# text = \"Good night ðŸ˜Š\"\n",
    "# encoded_input = tokenizer(text, return_tensors='tf')\n",
    "# output = model(encoded_input)\n",
    "# scores = output[0][0].numpy()\n",
    "# scores = softmax(scores)\n",
    "\n",
    "ranking = np.argsort(scores)\n",
    "ranking = ranking[::-1]\n",
    "for i in range(scores.shape[0]):\n",
    "    l = labels[ranking[i]]\n",
    "    s = scores[ranking[i]]\n",
    "    print(f\"{i+1}) {l} {np.round(float(s), 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cathedral-excellence",
   "metadata": {},
   "outputs": [],
   "source": [
    "#continue previous block\n",
    "#preprocess using roBERT, turn comment and submission combined data to feature, and roberta score\n",
    "import numpy as np \n",
    "import urllib.request \n",
    "import os \n",
    "import csv \n",
    "import requests \n",
    "import time\n",
    "import math\n",
    "import rando\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import re, string, unicodedata\n",
    "import nltk\n",
    "import contractions\n",
    "import inflect\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from string import punctuation\n",
    "from autocorrect import Speller\n",
    "from autocorrect import spell\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import heapq\n",
    "import pickle\n",
    "#import tensorflow_text\n",
    "#use = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual-large/3\")\n",
    "#stopword = stopwords.words(â€˜englishâ€™)\n",
    "import warnings\n",
    "\n",
    "path_preprocessed=\"C:\\\\Backup of covid project\\\\2cls_CScombined_negVSnonneg\\\\temp\\\\\"\n",
    "path_model=\"C:\\\\Backup of covid project\\\\2cls_CScombined_negVSnonneg\\\\\"\n",
    "path_upprocessed=\"C:\\\\Backup of covid project\\\\2cls_CScombined_negVSnonneg\\\\data\\\\\"\n",
    "\n",
    "os.chdir(path_model)\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "import csv\n",
    "import urllib.request\n",
    "\n",
    "# Preprocess text (username and link placeholders)\n",
    "def preprocess(text):\n",
    "    new_text = []\n",
    "    for t in text.split(\" \"):\n",
    "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
    "        t = 'http' if t.startswith('http') else t\n",
    "        new_text.append(t)\n",
    "    return \" \".join(new_text)\n",
    "\n",
    "years=['2019','2020']\n",
    "schools = [\"notredame\",\"uofm\",\"columbia\",\"dartmouth\",\"UCSD\",\"berkeley\",\"Harvard\",\"ucla\"]\n",
    "#schools = [\"columbia\",\"notredame\",\"uofm\"]\n",
    "label_group=[\"label\",\"unlabel\"]\n",
    "schoolyear=['dartmouth',\"notredame2019\",\"uofm2019\",\"columbia2019\",\"dartmouth2019\",\"UCSD2019\",\"berkeley2019\",\"Harvard2019\",\"ucla2019\",\n",
    "            \"notredame2020\",\"uofm2020\",\"columbia2020\",\"dartmouth2020\",\"UCSD2020\",\"berkeley2020\",\"Harvard2020\",\"ucla2020\"]\n",
    "\n",
    "#att_sub=[\"id\",\"selftext\",\"title\"]\n",
    "#drop_sub=[\"time\",\"author_fullname\",\"author\",\"subreddit\",\"subreddit_subscribers\",\"all_awardings\",\"allow_live_comments\",\"can_mod_post\",\"contest_mode\",\"domain\",\"gildings\",\"is_crosspostable\",\"is_meta\",\"is_original_content\",\"is_reddit_media_domain\",\"is_robot_indexable\",\"is_self\",\"is_video\",\"locked\",\"media_only\",\"no_follow\",\"num_comments\",\"num_crossposts\",\"over_18\",\"parent_whitelist_status\",\"pinned\",\"pwls\",\"score\",\"send_replies\",\"spoiler\",\"stickied\",\"thumbnail\",\"total_awards_received\",\"whitelist_status\",\"wls\"]\n",
    "att_com=[\"time\",\"id\",\"body\",\"Emotion\",\"Topic\"]\n",
    "drop_com=[\"link_id\",\"parent_id\",\"author_fullname\",\"author\",\"gildings\",\"score\",\"subreddit\",\"no_follow\",\"total_awards_received\",\"all_awardings\",\"is_submitter\",\"locked\",\"send_replies\",\"stickied\",]\n",
    "\n",
    "files=[]\n",
    "nb_cm=[0,0,0,0,0,0,0,0]\n",
    "y=''\n",
    "for s in schoolyear:\n",
    "        f=s+y+\".csv\"\n",
    "        os.chdir(path_upprocessed)\n",
    "        data=pd.read_csv(f,skip_blank_lines=True)\n",
    "        data=data[att_com]\n",
    "        data['emo_pred_pos']=0\n",
    "        data['emo_pred_neu']=0\n",
    "        data['emo_pred_neg']=0\n",
    "        \n",
    "        for d in range(len(data['id'])):\n",
    "            text = preprocess(data[\"body\"][d])\n",
    "            encoded_input = tokenizer(text, return_tensors='pt',max_length=512,truncation=True)\n",
    "            output = model(**encoded_input,output_hidden_states=True)\n",
    "            scores = output[0][0].detach().numpy()\n",
    "            scores = softmax(scores)\n",
    "            data.loc[d,'emo_pred_pos'] = round(float(scores[2]),4)\n",
    "            data.loc[d,'emo_pred_neu'] = round(float(scores[1]),4)\n",
    "            data.loc[d,'emo_pred_neg'] = round(float(scores[0]),4)\n",
    "            \n",
    "            \n",
    "            hidden_states=output[-1]#[1][0][0].shape\n",
    "            emb_torch=hidden_states[-1][0][0]\n",
    "            emb = emb_torch.cpu().detach().numpy()\n",
    "            os.chdir(path_preprocessed)\n",
    "            name=\"feature_\"+s+y+'.csv'\n",
    "            e=open(name, 'a',newline='')\n",
    "            with e:\n",
    "                writer = csv.writer(e, delimiter=',')\n",
    "                writer.writerow(emb)\n",
    "        name=s+y+\"_score.csv\"\n",
    "        os.chdir(path_preprocessed)\n",
    "        data.to_csv(name, sep=',',mode='a',header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "waiting-williams",
   "metadata": {},
   "outputs": [],
   "source": [
    "#form label for merged data\n",
    "import numpy as np \n",
    "import urllib.request \n",
    "import os \n",
    "import csv \n",
    "import requests \n",
    "import time\n",
    "import math\n",
    "import rando\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import math\n",
    "#stopword = stopwords.words(â€˜englishâ€™)\n",
    "import warnings\n",
    "import random\n",
    "import scipy.sparse\n",
    "\n",
    "#path_result='C:\\\\Backup of covid project\\\\2cls_CScombined_negVSnonneg\\\\results\\\\'\n",
    "#path_data=\"C:\\\\Backup of covid project\\\\2cls_CScombined_negVSnonneg\\\\data\\\\\"\n",
    "#path_code=\"C:\\\\Backup of covid project\\\\2cls_CScombined_negVSnonneg\\\\\"\n",
    "\n",
    "path_preprocessed=\"C:\\\\Backup of covid project\\\\2cls_CScombined_negVSnonneg\\\\data\\\\\"\n",
    "path_model=\"C:\\\\Backup of covid project\\\\2cls_CScombined_negVSnonneg\\\\\"\n",
    "path_upprocessed=\"C:\\\\Backup of covid project\\\\2cls_CScombined_negVSnonneg\\\\data\\\\\"\n",
    "os.chdir(path_model)\n",
    "\n",
    "years=['2019','2020']\n",
    "schools = [\"notredame\",\"uofm\",\"columbia\",\"dartmouth\",\"UCSD\",\"berkeley\",\"Harvard\",\"ucla\"]\n",
    "#schools = [\"dartmouth\"]\n",
    "\n",
    "drop_com=[\"link_id\",\"parent_id\",\"author_fullname\",\"author\",\"gildings\",\"score\",\"subreddit\",\"no_follow\",\"total_awards_received\",\"all_awardings\",\"is_submitter\",\"locked\",\"send_replies\",\"stickied\",]\n",
    "att_com=[\"time\",\"id\",\"body\",\"Emotion\",\"Topic\"]\n",
    "\n",
    "emo=[\"Very Positive\",\"Positive\",\"Neutral\",\"Negative\",\"Very negative\"]\n",
    "top=[\"Covid\",\"Academics\",\"Sports\",\"Campus/Students Life\",\"Social Media\",\"Religion\",\"Politics\",\"Others\"]\n",
    "\n",
    "for s in schools:\n",
    "    label_emt=[]\n",
    "    idx=0\n",
    "    for y in years:\n",
    "        f=s+y+\".csv\"\n",
    "        os.chdir(path_upprocessed)\n",
    "        data=pd.read_csv(f,skip_blank_lines=True)\n",
    "        data=data[att_com]\n",
    "        for d in range(len(data['id'])):\n",
    "            if isinstance(data['Emotion'][d],str)==False:\n",
    "                row=[0,0]\n",
    "            elif data['Emotion'][d]==emo[0]:\n",
    "                row=[0,1]\n",
    "            elif data['Emotion'][d]==emo[1]:\n",
    "                row=[0,1]\n",
    "            elif data['Emotion'][d]==emo[2]:\n",
    "                row=[0,1]\n",
    "            elif data['Emotion'][d]==emo[3]:\n",
    "                row=[1,0]\n",
    "            elif data['Emotion'][d]==emo[4]:\n",
    "                row=[1,0]\n",
    "                        \n",
    "            os.chdir(path_preprocessed)  \n",
    "            name=\"label_emt3_\"+s+y+\".csv\"\n",
    "            e=open(name, 'a',newline='')\n",
    "            with e:\n",
    "                writer = csv.writer(e, delimiter=',')\n",
    "                writer.writerow(row)\n",
    "\n",
    "#dartmouth\n",
    "f=\"dartmouth.csv\"\n",
    "os.chdir(path_upprocessed)\n",
    "data=pd.read_csv(f,skip_blank_lines=True)\n",
    "data=data[att_com]\n",
    "for d in range(len(data['id'])):\n",
    "    if isinstance(data['Emotion'][d],str)==False:\n",
    "        row=[0,0]\n",
    "    elif data['Emotion'][d]==emo[0]:\n",
    "        row=[0,1]\n",
    "    elif data['Emotion'][d]==emo[1]:\n",
    "        row=[0,1]\n",
    "    elif data['Emotion'][d]==emo[2]:\n",
    "        row=[0,1]\n",
    "    elif data['Emotion'][d]==emo[3]:\n",
    "        row=[1,0]\n",
    "    elif data['Emotion'][d]==emo[4]:\n",
    "        row=[1,0]\n",
    "                        \n",
    "    os.chdir(path_preprocessed)  \n",
    "    name=\"label_emt3_dartmouth\"+\".csv\"\n",
    "    e=open(name, 'a',newline='')\n",
    "    with e:\n",
    "        writer = csv.writer(e, delimiter=',')\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acceptable-addiction",
   "metadata": {},
   "outputs": [],
   "source": [
    "#form SYMMETRIC and ASYMMETRIEC adjacency matrix\n",
    "import numpy as np \n",
    "import urllib.request \n",
    "import os \n",
    "import csv \n",
    "import requests \n",
    "import time\n",
    "import math\n",
    "import rando\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import re, string, unicodedata\n",
    "import nltk\n",
    "import contractions\n",
    "import inflect\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from string import punctuation\n",
    "from autocorrect import Speller\n",
    "from autocorrect import spell\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import scipy.sparse\n",
    "#stopword = stopwords.words(â€˜englishâ€™)\n",
    "import warnings\n",
    "from scipy.sparse import csc_matrix\n",
    "from scipy import sparse\n",
    "from csv import reader\n",
    "\n",
    "path_preprocessed=\"C:\\\\Backup of covid project\\\\2cls_CScombined_negVSnonneg\\\\data\\\\\"\n",
    "path_model=\"C:\\\\Backup of covid project\\\\2cls_CScombined_negVSnonneg\\\\\"\n",
    "path_upprocessed=\"C:\\\\Backup of covid project\\\\2cls_CScombined_negVSnonneg\\\\data\\\\\"\n",
    "os.chdir(path_model)\n",
    "\n",
    "types=[\"comment\"]\n",
    "#years=['2020']#['2019','2020']\n",
    "years=['2019','2020']\n",
    "schools = [\"notredame\",\"uofm\",\"columbia\",\"dartmouth\",\"UCSD\",\"berkeley\",\"Harvard\",\"ucla\"]\n",
    "#schools = [\"notredame\"]\n",
    "\n",
    "drop_com=[\"time\",\"author\",\"gildings\",\"score\",\"subreddit\",\"no_follow\",\"total_awards_received\",\"all_awardings\",\"is_submitter\",\"locked\",\"send_replies\",\"stickied\",]\n",
    "att_com=[\"author_fullname\",\"body\",\"Emotion\",\"Topic\",\"id\",\"link_id\",\"parent_id\"]\n",
    "\n",
    "emo=[\"Very Positive\",\"Positive\",\"Neutral\",\"Negative\",\"Very negative\"]\n",
    "top=[\"Covid\",\"Academics\",\"Sports\",\"Campus/Students Life\",\"Social Media\",\"Religion\",\"Politics\",\"Others\"]\n",
    "    \n",
    "for s in schools:\n",
    "    for y in years:\n",
    "        dic_c={}\n",
    "        idx=0\n",
    "        f=s+y+\".csv\"\n",
    "        os.chdir(path_upprocessed)\n",
    "        data=pd.read_csv(f,skip_blank_lines=True)\n",
    "        n=len(data['id'])\n",
    "        for d in range(len(data['id'])):\n",
    "            if isinstance(data['link_id'][d],str)==False:\n",
    "                id_c=\"\\\"\"+\"t3_\"+data['id'][d][1:]\n",
    "            else:\n",
    "                id_c=\"\\\"\"+\"t1_\"+data['id'][d][1:]\n",
    "            dic_c[id_c]=idx\n",
    "            idx+=1\n",
    "                         \n",
    "        adj_cc=csc_matrix((n,n), dtype=np.int8)\n",
    "        for d in range(len(data['id'])):\n",
    "            if isinstance(data['link_id'][d],str)==False:\n",
    "                id_c=\"\\\"\"+\"t3_\"+data['id'][d][1:]\n",
    "            else:\n",
    "                id_c=\"\\\"\"+\"t1_\"+data['id'][d][1:]\n",
    "            parent=data['parent_id'][d]\n",
    "            if parent in dic_c:\n",
    "                adj_cc[dic_c[parent],dic_c[id_c]]=1\n",
    "                adj_cc[dic_c[id_c],dic_c[parent]]=1\n",
    "                \n",
    "            parent=data['link_id'][d]\n",
    "            if parent in dic_c:\n",
    "                adj_cc[dic_c[parent],dic_c[id_c]]=1\n",
    "                adj_cc[dic_c[id_c],dic_c[parent]]=1\n",
    "        os.chdir(path_preprocessed)\n",
    "        name=\"dic_Csym_\"+s+y+\".p\"\n",
    "        pickle.dump(dic_c,open(name,\"wb\"))\n",
    "        name=\"CCsym_\"+s+y+\".npz\"\n",
    "        sparse.save_npz(name, adj_cc)\n",
    "        \n",
    "        \n",
    "        adj_cc=csc_matrix((n,n), dtype=np.int8)\n",
    "        for d in range(len(data['id'])):\n",
    "            if isinstance(data['link_id'][d],str)==False:\n",
    "                id_c=\"\\\"\"+\"t3_\"+data['id'][d][1:]\n",
    "            else:\n",
    "                id_c=\"\\\"\"+\"t1_\"+data['id'][d][1:]\n",
    "            parent=data['parent_id'][d]\n",
    "            if parent in dic_c:\n",
    "                #adj_cc[dic_c[parent],dic_c[id_c]]=1\n",
    "                adj_cc[dic_c[id_c],dic_c[parent]]=1\n",
    "                \n",
    "            parent=data['link_id'][d]\n",
    "            if parent in dic_c:\n",
    "                #adj_cc[dic_c[parent],dic_c[id_c]]=1\n",
    "                adj_cc[dic_c[id_c],dic_c[parent]]=1\n",
    "        os.chdir(path_preprocessed)\n",
    "        name=\"dic_Casym_\"+s+y+\".p\"\n",
    "        pickle.dump(dic_c,open(name,\"wb\"))\n",
    "        name=\"CCasym_\"+s+y+\".npz\"\n",
    "        sparse.save_npz(name, adj_cc)\n",
    "        \n",
    "        \n",
    "        \n",
    "#dartmouth\n",
    "s='dartmouth'\n",
    "y=''\n",
    "dic_c={}\n",
    "idx=0\n",
    "f=s+y+\".csv\"\n",
    "os.chdir(path_upprocessed)\n",
    "data=pd.read_csv(f,skip_blank_lines=True)\n",
    "n=len(data['id'])\n",
    "for d in range(len(data['id'])):\n",
    "    if isinstance(data['link_id'][d],str)==False:\n",
    "        id_c=\"\\\"\"+\"t3_\"+data['id'][d][1:]\n",
    "    else:\n",
    "        id_c=\"\\\"\"+\"t1_\"+data['id'][d][1:]\n",
    "    dic_c[id_c]=idx\n",
    "    idx+=1\n",
    "                         \n",
    "adj_cc=csc_matrix((n,n), dtype=np.int8)\n",
    "for d in range(len(data['id'])):\n",
    "    if isinstance(data['link_id'][d],str)==False:\n",
    "        id_c=\"\\\"\"+\"t3_\"+data['id'][d][1:]\n",
    "    else:\n",
    "        id_c=\"\\\"\"+\"t1_\"+data['id'][d][1:]\n",
    "    parent=data['parent_id'][d]\n",
    "    if parent in dic_c:\n",
    "        adj_cc[dic_c[parent],dic_c[id_c]]=1\n",
    "        adj_cc[dic_c[id_c],dic_c[parent]]=1\n",
    "                \n",
    "    parent=data['link_id'][d]\n",
    "    if parent in dic_c:\n",
    "        adj_cc[dic_c[parent],dic_c[id_c]]=1\n",
    "        adj_cc[dic_c[id_c],dic_c[parent]]=1\n",
    "os.chdir(path_preprocessed)\n",
    "name=\"dic_Csym_\"+s+y+\".p\"\n",
    "pickle.dump(dic_c,open(name,\"wb\"))\n",
    "name=\"CCsym_\"+s+y+\".npz\"\n",
    "sparse.save_npz(name, adj_cc)\n",
    "        \n",
    "        \n",
    "adj_cc=csc_matrix((n,n), dtype=np.int8)\n",
    "for d in range(len(data['id'])):\n",
    "    if isinstance(data['link_id'][d],str)==False:\n",
    "        id_c=\"\\\"\"+\"t3_\"+data['id'][d][1:]\n",
    "    else:\n",
    "        id_c=\"\\\"\"+\"t1_\"+data['id'][d][1:]\n",
    "    parent=data['parent_id'][d]\n",
    "    if parent in dic_c:\n",
    "        #adj_cc[dic_c[parent],dic_c[id_c]]=1\n",
    "        adj_cc[dic_c[id_c],dic_c[parent]]=1\n",
    "                \n",
    "    parent=data['link_id'][d]\n",
    "    if parent in dic_c:\n",
    "        #adj_cc[dic_c[parent],dic_c[id_c]]=1\n",
    "        adj_cc[dic_c[id_c],dic_c[parent]]=1\n",
    "os.chdir(path_preprocessed)\n",
    "name=\"dic_Casym_\"+s+y+\".p\"\n",
    "pickle.dump(dic_c,open(name,\"wb\"))\n",
    "name=\"CCasym_\"+s+y+\".npz\"\n",
    "sparse.save_npz(name, adj_cc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compact-isolation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#assigning index\n",
    "#index generating for neg VS non-neg\n",
    "import numpy as np \n",
    "import urllib.request \n",
    "import os \n",
    "import csv \n",
    "import requests \n",
    "import time\n",
    "import math\n",
    "import rando\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import re, string, unicodedata\n",
    "import nltk\n",
    "import contractions\n",
    "import inflect\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from string import punctuation\n",
    "from autocorrect import Speller\n",
    "from autocorrect import spell\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import math\n",
    "#stopword = stopwords.words(â€˜englishâ€™)\n",
    "import warnings\n",
    "import random\n",
    "import scipy.sparse\n",
    "\n",
    "#path_result='C:\\\\Backup of covid project\\\\2cls_posVSnonpos\\\\results\\\\'\n",
    "#path_data=\"C:\\\\Backup of covid project\\\\2cls_posVSnonpos\\\\data\\\\\"\n",
    "#path_code=\"C:\\\\Backup of covid project\\\\\"\n",
    "\n",
    "path_result=\"C:\\\\Backup of covid project\\\\2cls_CScombined_negVSnonneg\\\\data\\\\\"\n",
    "path_data=\"C:\\\\Backup of covid project\\\\2cls_CScombined_negVSnonneg\\\\data\"\n",
    "path_code=\"C:\\\\Backup of covid project\\\\2cls_CScombined_negVSnonneg\\\\data\\\\\"\n",
    "\n",
    "#path_result='C:\\\\Backup of covid project\\\\2cls_wlNeu_woNeu\\\\results_WOneu\\\\'\n",
    "#path_data=\"C:\\\\Backup of covid project\\\\2cls_wlNeu_woNeu\\\\data\\\\\"\n",
    "#path_code=\"C:\\\\Backup of covid project\\\\\"\n",
    "\n",
    "random.seed(10)\n",
    "\n",
    "types=[\"comment\"]\n",
    "years=['2019','2020']\n",
    "\n",
    "label_group=[\"label\",\"unlabel\"]\n",
    "months=['08','09','10','11']\n",
    "\n",
    "drop_com=[\"link_id\",\"parent_id\",\"author_fullname\",\"author\",\"gildings\",\"score\",\"subreddit\",\"no_follow\",\"total_awards_received\",\"all_awardings\",\"is_submitter\",\"locked\",\"send_replies\",\"stickied\",]\n",
    "att_com=[\"time\",\"id\",\"body\",\"Emotion\",\"Topic\"]\n",
    "\n",
    "#emo=[\"Very Positive\",\"Positive\",\"Neutral\",\"Negative\",\"Very negative\"]\n",
    "emo=[\"Very negative\",\"Negative\",\"Neutral\",\"Positive\",\"Very Positive\"]\n",
    "top=[\"Covid\",\"Academics\",\"Sports\",\"Campus/Students Life\",\"Social Media\",\"Religion\",\"Politics\",\"Others\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#FIRST DEAL WITH DARTMOUTH DATA\n",
    "#schools = [\"notredame\",\"uofm\",\"columbia\",\"dartmouth\",\"UCSD\",\"berkeley\",\"Harvard\",\"ucla\"]\n",
    "schools = [\"dartmouth\"]\n",
    "for s in schools:\n",
    "    train_index=[]\n",
    "    test_index=[]\n",
    "    to_be_labeled_index=[]\n",
    "    idx=0\n",
    "    n_test=[50,31,19]#this number is derived from proportion of neg, neu, and nonneg\n",
    "    f=s+\".csv\"\n",
    "    os.chdir(path_data)\n",
    "    data=pd.read_csv(f,skip_blank_lines=True)\n",
    "    for d in range(len(data['id'])):\n",
    "        if isinstance(data['Emotion'][d],str)==False:\n",
    "            to_be_labeled_index.append(idx)\n",
    "        elif data['Emotion'][d]==emo[0] and n_test[0]>0:\n",
    "            test_index.append(idx)\n",
    "            n_test[0]=n_test[0]-1\n",
    "        elif data['Emotion'][d]==emo[0]:\n",
    "            train_index.append(idx)\n",
    "        elif data['Emotion'][d]==emo[1] and n_test[0]>0:\n",
    "            test_index.append(idx)\n",
    "            n_test[0]=n_test[0]-1\n",
    "        elif data['Emotion'][d]==emo[1]:\n",
    "            train_index.append(idx)\n",
    "        elif data['Emotion'][d]==emo[2] and n_test[1]>0:\n",
    "            test_index.append(idx)\n",
    "            n_test[1]=n_test[1]-1\n",
    "        elif data['Emotion'][d]==emo[2]:\n",
    "            train_index.append(idx)\n",
    "        elif data['Emotion'][d]==emo[3] and n_test[2]>0:\n",
    "            test_index.append(idx)\n",
    "            n_test[2]=n_test[2]-1\n",
    "        elif data['Emotion'][d]==emo[3]:\n",
    "            train_index.append(idx)\n",
    "        elif data['Emotion'][d]==emo[4] and n_test[2]>0:\n",
    "            test_index.append(idx)\n",
    "            n_test[2]=n_test[2]-1\n",
    "        elif data['Emotion'][d]==emo[4]:\n",
    "            train_index.append(idx)\n",
    "        else:\n",
    "            to_be_labeled_index.append(idx)\n",
    "        idx+=1\n",
    "            \n",
    "                           \n",
    "    os.chdir(path_result)\n",
    "    name=\"train_index_\"+s+\"_cm.p\"\n",
    "    pickle.dump(train_index,open(name,\"wb\"))\n",
    "    name=\"test_index_\"+s+\"_cm.p\"\n",
    "    pickle.dump(test_index,open(name,\"wb\"))\n",
    "    name=\"to_be_labeled_index_\"+s+\"_cm.p\"\n",
    "    pickle.dump(to_be_labeled_index,open(name,\"wb\"))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "#FIRST DEAL WITH DARTMOUTH DATA\n",
    "schools = [\"notredame\",\"uofm\",\"columbia\",\"UCSD\",\"berkeley\",\"Harvard\",\"ucla\",'dartmouth']\n",
    "#schools = [\"dartmouth\"]\n",
    "for s in schools:\n",
    "    for y in years:\n",
    "        train_index=[]\n",
    "        test_index=[]\n",
    "        to_be_labeled_index=[]\n",
    "        idx=0\n",
    "        n_test=[50,31,19]#this number is derived from proportion of neg, neu, and nonneg\n",
    "        f=s+y+\".csv\"\n",
    "        os.chdir(path_data)\n",
    "        data=pd.read_csv(f,skip_blank_lines=True)\n",
    "        for d in range(len(data['id'])):\n",
    "            if isinstance(data['Emotion'][d],str)==False:\n",
    "                to_be_labeled_index.append(idx)\n",
    "            elif data['Emotion'][d]==emo[0] and n_test[0]>0:\n",
    "                test_index.append(idx)\n",
    "                n_test[0]=n_test[0]-1\n",
    "            elif data['Emotion'][d]==emo[0]:\n",
    "                train_index.append(idx)\n",
    "            elif data['Emotion'][d]==emo[1] and n_test[0]>0:\n",
    "                test_index.append(idx)\n",
    "                n_test[0]=n_test[0]-1\n",
    "            elif data['Emotion'][d]==emo[1]:\n",
    "                train_index.append(idx)\n",
    "            elif data['Emotion'][d]==emo[2] and n_test[1]>0:\n",
    "                test_index.append(idx)\n",
    "                n_test[1]=n_test[1]-1\n",
    "            elif data['Emotion'][d]==emo[2]:\n",
    "                to_be_labeled_index.append(idx)\n",
    "            elif data['Emotion'][d]==emo[3] and n_test[2]>0:\n",
    "                test_index.append(idx)\n",
    "                n_test[2]=n_test[2]-1\n",
    "            elif data['Emotion'][d]==emo[3]:\n",
    "                train_index.append(idx)\n",
    "            elif data['Emotion'][d]==emo[4] and n_test[2]>0:\n",
    "                test_index.append(idx)\n",
    "                n_test[2]=n_test[2]-1\n",
    "            elif data['Emotion'][d]==emo[4]:\n",
    "                train_index.append(idx)\n",
    "            else:\n",
    "                to_be_labeled_index.append(idx)\n",
    "            idx+=1\n",
    "        os.chdir(path_result)\n",
    "        name=\"train_index_\"+s+y+\"_cm.p\"\n",
    "        pickle.dump(train_index,open(name,\"wb\"))\n",
    "        name=\"test_index_\"+s+y+\"_cm.p\"\n",
    "        pickle.dump(test_index,open(name,\"wb\"))\n",
    "        name=\"to_be_labeled_index_\"+s+y+\"_cm.p\"\n",
    "        pickle.dump(to_be_labeled_index,open(name,\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supported-entrepreneur",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
