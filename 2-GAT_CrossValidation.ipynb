{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respiratory-pierce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross validation. From a set of candidate parameters, see which parameter set performs the best s\n",
    "#make sure model code (gat, layers, base_gattn) is in path \"path_code\" \n",
    "#Code in this nodebook is modified from: https://github.com/Jhy1993/HAN \n",
    "\n",
    "path_result='C:\\\\Backup of covid project\\\\2cls_negVSnonneg\\\\results\\\\'\n",
    "path_data=\"C:\\\\Backup of covid project\\\\2cls_negVSnonneg\\\\data\\\\\"\n",
    "path_code=\"C:\\\\Backup of covid project\\\\\"\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import random\n",
    "os.chdir(path_code)\n",
    "import gat\n",
    "import imp\n",
    "imp.reload(gat)\n",
    "from gat import GAT, HeteGAT, HeteGAT_multi  # or * for that matter\n",
    "import process\n",
    "import importlib\n",
    "importlib.reload(process)\n",
    "import numpy as np\n",
    "import pickle\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2,3\"\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "# jhy data\n",
    "import scipy.io as sio\n",
    "import scipy.sparse as sp\n",
    "\n",
    "\n",
    "def sample_mask(idx, l):\n",
    "    \"\"\"Create mask.\"\"\"\n",
    "    mask = np.zeros(l)\n",
    "    mask[idx] = 1\n",
    "    return np.array(mask, dtype=np.bool)\n",
    "\n",
    "def load_data_dblp(fold,path=path_data):\n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "    import scipy.sparse\n",
    "    from scipy.sparse import csc_matrix\n",
    "    from scipy import sparse\n",
    "    import numpy as np \n",
    "    import os \n",
    "    import csv \n",
    "    import numpy as np\n",
    "    schools = [\"notredame\",\"uofm\",\"columbia\",\"dartmouth\",\"UCSD\",\"berkeley\",\"Harvard\",\"ucla\"]\n",
    "    s=schools[3]#use this index to control which network we are going to run.\n",
    "    t=\"comment\"\n",
    "    \n",
    "    os.chdir(path)\n",
    "    name=\"label_emt3_\"+s+\"_cm.csv\"\n",
    "    truelabels=pd.read_csv(name,skip_blank_lines=True,header=None)\n",
    "    name=\"feature_\"+s+t+\".csv\"\n",
    "    truefeatures=pd.read_csv(name,skip_blank_lines=True,header=None).values\n",
    "    N=truefeatures.shape[0]\n",
    "    name=\"CCsym_\"+s+\".npz\"\n",
    "    dat_cc=scipy.sparse.load_npz(name);dat_cc=dat_cc.toarray();\n",
    "    rownetworks = [(dat_cc)]\n",
    "\n",
    "    y=truelabels    \n",
    "    name=\"train_index_\"+s+\"_cm.p\"\n",
    "    train_idx = pickle.load(open(name,\"rb\"));train_idx=np.array(train_idx)\n",
    "    name=\"to_be_labeled_index_\"+s+\"_cm.p\"\n",
    "    val_idx = pickle.load(open(name,\"rb\"));val_idx=np.array(val_idx)\n",
    "    name=\"test_index_\"+s+\"_cm.p\"\n",
    "    #name=\"test_index_\"+s+y+\"_cm.p\"#use this to do 3 class CV\n",
    "    test_idx = pickle.load(open(name,\"rb\"));test_idx=np.array(test_idx)\n",
    "    import random\n",
    "    random.seed(1)\n",
    "    random.shuffle(train_idx)\n",
    "    indexes=[]\n",
    "    temp=sample_mask(train_idx, y.shape[0])\n",
    "    tt=y.iloc[temp, :]\n",
    "    \n",
    "    t=np.sum(tt,axis=0)\n",
    "    n_neg=t[0]\n",
    "    n_pos=t[1]\n",
    "    train_idx=tuple(train_idx)\n",
    "    train_idx_tm=list(train_idx)\n",
    "    for k in range(kfolds):\n",
    "    \n",
    "        bgt_neg=round(n_neg/kfolds)\n",
    "        bgt_pos=round(n_pos/kfolds)\n",
    "        if k==kfolds-1:\n",
    "            bgt_neg=n_neg-k*round(n_neg/kfolds)\n",
    "            bgt_pos=n_pos-k*round(n_pos/kfolds)\n",
    "        group_index=[]\n",
    "        for train in train_idx:\n",
    "            if list(y.iloc[train,]).index(1)==0 and bgt_neg>0:\n",
    "                group_index.append(train)\n",
    "                train_idx_tm.remove(train)\n",
    "                bgt_neg-=1\n",
    "            elif list(y.iloc[train,]).index(1)==1 and bgt_pos>0:\n",
    "                group_index.append(train)\n",
    "                train_idx_tm.remove(train)\n",
    "                bgt_pos-=1\n",
    "        train_idx=tuple(train_idx_tm)\n",
    "        indexes.append(group_index)\n",
    "    \n",
    "    val_idx=np.array(indexes[fold])\n",
    "    import itertools\n",
    "    train_idx=list(itertools.chain.from_iterable(indexes))\n",
    "    for t in val_idx: train_idx.remove(t)\n",
    "    train_idx=np.array(train_idx)\n",
    "    \n",
    "    train_mask = sample_mask(train_idx, y.shape[0])\n",
    "    val_mask = sample_mask(val_idx, y.shape[0])\n",
    "    test_mask = sample_mask(test_idx, y.shape[0])\n",
    "    \n",
    "    y_train = np.zeros(y.shape)\n",
    "    y_val = np.zeros(y.shape)\n",
    "    y_test = np.zeros(y.shape)\n",
    "    y=y.values\n",
    "    y_train[train_mask, :] = y[train_mask, :]\n",
    "    y_val[val_mask, :] = y[val_mask, :]\n",
    "    y_test[test_mask, :] = y[test_mask, :]\n",
    "    truefeatures_list = [truefeatures, truefeatures]#, truefeatures, truefeatures]# ?? why copy three times? First for center node, 2 for each metapath.    \n",
    "    os.chdir(path_code)\n",
    "    return rownetworks, truefeatures_list, y_train, y_val, y_val, train_mask, val_mask, val_mask\n",
    "\n",
    "def HAN(nb_epochs= 100,l2_coef= 0.05,lr= 0.005,hid_units= [8],n_heads= [4, 1] ,ffd_drop_set=0,attn_drop_set=0,fold=0, weight=1):\n",
    "\n",
    "    dataset = 'acm'\n",
    "    featype = 'fea'\n",
    "    checkpt_file = path_result+'acm_allMP_multi_fea_2cls.ckpt'\n",
    "    batch_size = 1\n",
    "    patience = 100\n",
    "    residual = False\n",
    "    nonlinearity = tf.nn.elu\n",
    "    model = HeteGAT_multi\n",
    "\n",
    "    adj_list, fea_list, y_train, y_val, y_test, train_mask, val_mask, test_mask = load_data_dblp(fold=fold)\n",
    "    if featype == 'adj':\n",
    "        fea_list = adj_list\n",
    "    nb_nodes = fea_list[0].shape[0]\n",
    "    ft_size = fea_list[0].shape[1]\n",
    "    nb_classes = y_train.shape[1]\n",
    "\n",
    "    fea_list = [fea[np.newaxis] for fea in fea_list]\n",
    "    adj_list = [adj[np.newaxis] for adj in adj_list]\n",
    "    y_train = y_train[np.newaxis]\n",
    "    y_val = y_val[np.newaxis]\n",
    "    y_test = y_test[np.newaxis]\n",
    "    train_mask = train_mask[np.newaxis]\n",
    "    val_mask = val_mask[np.newaxis]\n",
    "    test_mask = test_mask[np.newaxis]\n",
    "\n",
    "    biases_list = [process.adj_to_bias(adj, [nb_nodes], nhood=1) for adj in adj_list]\n",
    "    with tf.Graph().as_default():\n",
    "        with tf.name_scope('input'):\n",
    "            ftr_in_list = [tf.placeholder(dtype=tf.float32,\n",
    "                                          shape=(batch_size, nb_nodes, ft_size),\n",
    "                                          name='ftr_in_{}'.format(i))\n",
    "                           for i in range(len(fea_list))]\n",
    "            bias_in_list = [tf.placeholder(dtype=tf.float32,\n",
    "                                           shape=(batch_size, nb_nodes, nb_nodes),\n",
    "                                           name='bias_in_{}'.format(i))\n",
    "                            for i in range(len(biases_list))]\n",
    "            lbl_in = tf.placeholder(dtype=tf.int32, shape=(\n",
    "                batch_size, nb_nodes, nb_classes), name='lbl_in')\n",
    "            msk_in = tf.placeholder(dtype=tf.int32, shape=(batch_size, nb_nodes),\n",
    "                                    name='msk_in')\n",
    "            attn_drop = tf.placeholder(dtype=tf.float32, shape=(), name='attn_drop')\n",
    "            ffd_drop = tf.placeholder(dtype=tf.float32, shape=(), name='ffd_drop')\n",
    "            is_train = tf.placeholder(dtype=tf.bool, shape=(), name='is_train')\n",
    "        # forward\n",
    "        logits, final_embedding, att_val = model.inference(ftr_in_list, nb_classes, nb_nodes, is_train,\n",
    "                                                       attn_drop, ffd_drop,\n",
    "                                                       bias_mat_list=bias_in_list,\n",
    "                                                       hid_units=hid_units, n_heads=n_heads,\n",
    "                                                       residual=residual, activation=nonlinearity)\n",
    "\n",
    "        # cal masked_loss\n",
    "        log_resh = tf.reshape(logits, [-1, nb_classes])\n",
    "        lab_resh = tf.reshape(lbl_in, [-1, nb_classes])\n",
    "        msk_resh = tf.reshape(msk_in, [-1])\n",
    "        loss = model.masked_softmax_cross_entropy(log_resh, lab_resh, msk_resh, weight)\n",
    "        predicted=tf.argmax(log_resh, 1)\n",
    "        accuracy = model.masked_accuracy(log_resh, lab_resh, msk_resh)\n",
    "        # optimzie\n",
    "        train_op = model.training(loss, lr, l2_coef)\n",
    "    \n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        init_op = tf.group(tf.global_variables_initializer(),\n",
    "                       tf.local_variables_initializer())\n",
    "\n",
    "        vlss_mn = np.inf\n",
    "        vacc_mx = 0.0\n",
    "        curr_step = 0\n",
    "\n",
    "        with tf.Session(config=config) as sess:\n",
    "            sess.run(init_op)\n",
    "\n",
    "            train_loss_avg = 0\n",
    "            train_acc_avg = 0\n",
    "            val_loss_avg = 0\n",
    "            val_acc_avg = 0\n",
    "            val_pred_avg = 0\n",
    "            for epoch in range(nb_epochs):\n",
    "                tr_step = 0\n",
    "           \n",
    "                tr_size = fea_list[0].shape[0]\n",
    "                # ================   training    ============\n",
    "                while tr_step * batch_size < tr_size:\n",
    "\n",
    "                    fd1 = {i: d[tr_step * batch_size:(tr_step + 1) * batch_size]\n",
    "                           for i, d in zip(ftr_in_list, fea_list)}\n",
    "                    fd2 = {i: d[tr_step * batch_size:(tr_step + 1) * batch_size]\n",
    "                           for i, d in zip(bias_in_list, biases_list)}\n",
    "                    fd3 = {lbl_in: y_train[tr_step * batch_size:(tr_step + 1) * batch_size],\n",
    "                       msk_in: train_mask[tr_step * batch_size:(tr_step + 1) * batch_size],\n",
    "                       is_train: True,\n",
    "                       attn_drop: attn_drop_set,\n",
    "                       ffd_drop: ffd_drop_set}\n",
    "                    fd = fd1\n",
    "                    fd.update(fd2)\n",
    "                    fd.update(fd3)\n",
    "                    _, loss_value_tr, acc_tr, att_val_train = sess.run([train_op, loss, accuracy, att_val],\n",
    "                                                                   feed_dict=fd)\n",
    "                    train_loss_avg += loss_value_tr\n",
    "                    train_acc_avg += acc_tr\n",
    "                    tr_step += 1\n",
    "\n",
    "                vl_step = 0\n",
    "                vl_size = fea_list[0].shape[0]\n",
    "                curr_step += 1\n",
    "                if epoch==nb_epochs-1:\n",
    "                    saver.save(sess, checkpt_file)\n",
    "\n",
    "                train_loss_avg = 0\n",
    "                train_acc_avg = 0\n",
    "                val_loss_avg = 0\n",
    "                val_acc_avg = 0\n",
    "            saver.restore(sess, checkpt_file)\n",
    "            ts_size = fea_list[0].shape[0]\n",
    "            ts_step = 0\n",
    "            ts_loss = 0.0\n",
    "            ts_acc = 0.0\n",
    "\n",
    "            while ts_step * batch_size < ts_size:\n",
    "                fd1 = {i: d[ts_step * batch_size:(ts_step + 1) * batch_size]\n",
    "                       for i, d in zip(ftr_in_list, fea_list)}\n",
    "                fd2 = {i: d[ts_step * batch_size:(ts_step + 1) * batch_size]\n",
    "                       for i, d in zip(bias_in_list, biases_list)}\n",
    "                fd3 = {lbl_in: y_test[ts_step * batch_size:(ts_step + 1) * batch_size],\n",
    "                       msk_in: test_mask[ts_step * batch_size:(ts_step + 1) * batch_size],\n",
    "            \n",
    "                       is_train: False,\n",
    "                       attn_drop: attn_drop_set,\n",
    "                       ffd_drop: ffd_drop_set}\n",
    "        \n",
    "                fd = fd1\n",
    "                fd.update(fd2)\n",
    "                fd.update(fd3)\n",
    "                loss_value_ts, acc_ts, jhy_final_embedding = sess.run([loss, accuracy, final_embedding],\n",
    "                                                                  feed_dict=fd)\n",
    "                ts_loss += loss_value_ts\n",
    "                ts_acc += acc_ts\n",
    "                ts_step += 1\n",
    "\n",
    "            print('Test loss:', ts_loss / ts_step,\n",
    "              '; Test accuracy:', ts_acc / ts_step)\n",
    "            \n",
    "            #crs.append(ts_loss / ts_step)\n",
    "            #crs=tuple(crs)\n",
    "            import pickle\n",
    "            \n",
    "\n",
    "            sess.close()\n",
    "            return(ts_loss / ts_step)\n",
    "\n",
    "l2_candi=list([0.02, 0.05,0.08,0.1,0.2])\n",
    "lr_candi=list([0.02])\n",
    "weight_candi=list([2])\n",
    "\n",
    "repeats=10\n",
    "kfolds=4\n",
    "\n",
    "records=[]\n",
    "count=0\n",
    "for p1 in l2_candi:\n",
    "    for p2 in lr_candi:\n",
    "        for p3 in weight_candi:\n",
    "            crs_rp=[]\n",
    "            for r in range(repeats):\n",
    "                crs=[]\n",
    "                for fold in range(kfolds): \n",
    "                    imp.reload(gat)\n",
    "                    from gat import GAT, HeteGAT, HeteGAT_multi  # or * for that matter\n",
    "                    cr=HAN(nb_epochs=100,l2_coef= p1,lr= p2,hid_units= [8],n_heads= [4, 1] ,ffd_drop_set=0,attn_drop_set=0,fold=fold,weight=p3)\n",
    "                    count+=1\n",
    "                    crs.append(cr)\n",
    "                crs_rp.append(crs)\n",
    "            records.append(crs_rp)\n",
    "            print('Finished running one parameter set @_@')\n",
    "            \n",
    "print(count)           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "micro-dylan",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copy paste print-outs from last block to \"xvali.txt\" in your \"path_result\" and save it before running this block!\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "#path_result='C:\\\\Backup of covid project\\\\2cls_posVSnonpos\\\\results\\\\'\n",
    "path_result='C:\\\\Backup of covid project\\\\2cls_negVSnonneg\\\\results\\\\'\n",
    "#path_result='C:\\\\Backup of covid project\\\\2cls_wlNeu_woNeu\\\\results_WOneu\\\\'\n",
    "\n",
    "os.chdir(path_result)\n",
    "\n",
    "file1 = open('xvali.txt', 'r')\n",
    "Lines = file1.readlines()\n",
    "new_lines=[a.replace('\\n','').replace('[','').replace(']','').replace(',','') for a in Lines if a!='\\n']\n",
    "print(len(new_lines))\n",
    "new_lines1=[]\n",
    "for a in range(len(new_lines)):\n",
    "    if new_lines[a][0:4]=='Test':\n",
    "        new_lines1.append(new_lines[a])\n",
    "new_lines2=[]\n",
    "print(len(new_lines1))\n",
    "for a in range(len(new_lines1)):\n",
    "    acc=float(new_lines1[a].split('Test accuracy: ')[1])\n",
    "    new_lines2.append(acc)\n",
    "print(len(new_lines2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chicken-benchmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "#See which parameter combination perform the best\n",
    "records_agg=[]\n",
    "\n",
    "#repeats=10\n",
    "#kfolds=4\n",
    "for a1 in range(len(l2_candi)):\n",
    "    r1=[]\n",
    "    for a2 in range(len(lr_candi)):\n",
    "        r2=[]\n",
    "        for a3 in range(len(weight_candi)):\n",
    "            r3=[]\n",
    "            for a4 in range(repeats):\n",
    "                r4=[]\n",
    "                for a5 in range(kfolds): \n",
    "                    r4.append(new_lines2[0])\n",
    "                    new_lines2.pop(0)\n",
    "                r3.append(r4)\n",
    "            r2.append(r3)\n",
    "        r1.append(r2)\n",
    "    records_agg.append(r1)\n",
    "\n",
    "\n",
    "r=np.array(records_agg,dtype='float')\n",
    "print(r.shape)\n",
    "#print(r)\n",
    "w=np.mean(r,axis=3)\n",
    "print(w.shape)\n",
    "e=np.mean(w,axis=3)\n",
    "print(e.shape)\n",
    "pos=np.where(e== e.max())\n",
    "print(e)\n",
    "print(pos)\n",
    "\n",
    "print('l2:'+str(l2_candi[pos[0][0]]))\n",
    "print('lr:'+str(lr_candi[pos[1][0]]))\n",
    "print('weight:'+str(weight_candi[pos[2][0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "laden-phrase",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(weight_candi)\n",
    "print((lr_candi))\n",
    "print(l2_candi)\n",
    "\n",
    "print(repeats)\n",
    "print(kfolds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
