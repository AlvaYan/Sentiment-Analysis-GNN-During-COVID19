{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: C:\\Backup of covid project\\2cls_negVSnonneg\\results\\acm_allMP_multi_fea_.ckpt\n",
      "Dataset: acm\n",
      "----- Opt. hyperparams -----\n",
      "lr: 0.02\n",
      "l2_coef: 0.08\n",
      "----- Archi. hyperparams -----\n",
      "nb. layers: 1\n",
      "nb. units per layer: [8]\n",
      "nb. attention heads: [4, 1]\n",
      "residual: False\n",
      "nonlinearity: <function elu at 0x000001A1544A9E58>\n",
      "model: <class 'gat.HeteGAT_multi'>\n",
      "y_train:(2009, 2), y_val:(2009, 2), y_test:(2009, 2), train_idx:(417,), val_idx:(1492,), test_idx:(100,)\n",
      "build graph...\n",
      "de\n",
      "Epoch: 0, att_val: [1.]\n",
      "Training: loss = 1.29802, acc = 0.61151 |\n",
      "Epoch: 1, att_val: [1.]\n",
      "Training: loss = 2.54631, acc = 0.70983 |\n",
      "Epoch: 2, att_val: [1.]\n",
      "Training: loss = 2.27496, acc = 0.76978 |\n",
      "Epoch: 3, att_val: [1.]\n",
      "Training: loss = 1.81248, acc = 0.81295 |\n",
      "Epoch: 4, att_val: [1.]\n",
      "Training: loss = 1.34823, acc = 0.83213 |\n",
      "Epoch: 5, att_val: [1.]\n",
      "Training: loss = 0.94134, acc = 0.83933 |\n",
      "Epoch: 6, att_val: [1.]\n",
      "Training: loss = 0.62805, acc = 0.84413 |\n",
      "Epoch: 7, att_val: [1.]\n",
      "Training: loss = 0.42898, acc = 0.82734 |\n",
      "Epoch: 8, att_val: [1.]\n",
      "Training: loss = 0.43400, acc = 0.76739 |\n",
      "Epoch: 9, att_val: [1.]\n",
      "Training: loss = 0.46632, acc = 0.80336 |\n",
      "Epoch: 10, att_val: [1.]\n",
      "Training: loss = 0.42347, acc = 0.85372 |\n",
      "Epoch: 11, att_val: [1.]\n",
      "Training: loss = 0.37668, acc = 0.88010 |\n",
      "Epoch: 12, att_val: [1.]\n",
      "Training: loss = 0.36644, acc = 0.79616 |\n",
      "Epoch: 13, att_val: [1.]\n",
      "Training: loss = 0.37590, acc = 0.77698 |\n",
      "Epoch: 14, att_val: [1.]\n",
      "Training: loss = 0.36125, acc = 0.80096 |\n",
      "Epoch: 15, att_val: [1.]\n",
      "Training: loss = 0.34846, acc = 0.83693 |\n",
      "Epoch: 16, att_val: [1.]\n",
      "Training: loss = 0.35045, acc = 0.86571 |\n",
      "Epoch: 17, att_val: [1.]\n",
      "Training: loss = 0.34913, acc = 0.87050 |\n",
      "Epoch: 18, att_val: [1.]\n",
      "Training: loss = 0.34095, acc = 0.85612 |\n",
      "Epoch: 19, att_val: [1.]\n",
      "Training: loss = 0.34140, acc = 0.82494 |\n",
      "Epoch: 20, att_val: [1.]\n",
      "Training: loss = 0.34573, acc = 0.82014 |\n",
      "Epoch: 21, att_val: [1.]\n",
      "Training: loss = 0.34085, acc = 0.83453 |\n",
      "Epoch: 22, att_val: [1.]\n",
      "Training: loss = 0.33968, acc = 0.86091 |\n",
      "Epoch: 23, att_val: [1.]\n",
      "Training: loss = 0.34380, acc = 0.86571 |\n",
      "Epoch: 24, att_val: [1.]\n",
      "Training: loss = 0.34036, acc = 0.86331 |\n",
      "Epoch: 25, att_val: [1.]\n",
      "Training: loss = 0.33763, acc = 0.84173 |\n",
      "Epoch: 26, att_val: [1.]\n",
      "Training: loss = 0.34347, acc = 0.81295 |\n",
      "Epoch: 27, att_val: [1.]\n",
      "Training: loss = 0.33649, acc = 0.85132 |\n",
      "Epoch: 28, att_val: [1.]\n",
      "Training: loss = 0.33882, acc = 0.86811 |\n",
      "Epoch: 29, att_val: [1.]\n",
      "Training: loss = 0.32726, acc = 0.86091 |\n",
      "Epoch: 30, att_val: [1.]\n",
      "Training: loss = 0.32862, acc = 0.84173 |\n",
      "Epoch: 31, att_val: [1.]\n",
      "Training: loss = 0.32230, acc = 0.85612 |\n",
      "Epoch: 32, att_val: [1.]\n",
      "Training: loss = 0.32503, acc = 0.87050 |\n",
      "Epoch: 33, att_val: [1.]\n",
      "Training: loss = 0.31959, acc = 0.86091 |\n",
      "Epoch: 34, att_val: [1.]\n",
      "Training: loss = 0.32463, acc = 0.85132 |\n",
      "Epoch: 35, att_val: [1.]\n",
      "Training: loss = 0.32093, acc = 0.87050 |\n",
      "Epoch: 36, att_val: [1.]\n",
      "Training: loss = 0.31882, acc = 0.86811 |\n",
      "Epoch: 37, att_val: [1.]\n",
      "Training: loss = 0.32113, acc = 0.84413 |\n",
      "Epoch: 38, att_val: [1.]\n",
      "Training: loss = 0.31634, acc = 0.86091 |\n",
      "Epoch: 39, att_val: [1.]\n",
      "Training: loss = 0.31806, acc = 0.86811 |\n",
      "Epoch: 40, att_val: [1.]\n",
      "Training: loss = 0.31760, acc = 0.85132 |\n",
      "Epoch: 41, att_val: [1.]\n",
      "Training: loss = 0.31408, acc = 0.86091 |\n",
      "Epoch: 42, att_val: [1.]\n",
      "Training: loss = 0.31497, acc = 0.86571 |\n",
      "Epoch: 43, att_val: [1.]\n",
      "Training: loss = 0.31428, acc = 0.85851 |\n",
      "Epoch: 44, att_val: [1.]\n",
      "Training: loss = 0.31234, acc = 0.86091 |\n",
      "Epoch: 45, att_val: [1.]\n",
      "Training: loss = 0.31361, acc = 0.86571 |\n",
      "Epoch: 46, att_val: [1.]\n",
      "Training: loss = 0.31479, acc = 0.86091 |\n",
      "Epoch: 47, att_val: [1.]\n",
      "Training: loss = 0.31340, acc = 0.86571 |\n",
      "Epoch: 48, att_val: [1.]\n",
      "Training: loss = 0.31148, acc = 0.86091 |\n",
      "Epoch: 49, att_val: [1.]\n",
      "Training: loss = 0.31121, acc = 0.86091 |\n",
      "Epoch: 50, att_val: [1.]\n",
      "Training: loss = 0.31154, acc = 0.86811 |\n",
      "Epoch: 51, att_val: [1.]\n",
      "Training: loss = 0.31116, acc = 0.86331 |\n",
      "Epoch: 52, att_val: [1.]\n",
      "Training: loss = 0.31045, acc = 0.87050 |\n",
      "Epoch: 53, att_val: [1.]\n",
      "Training: loss = 0.30927, acc = 0.86331 |\n",
      "Epoch: 54, att_val: [1.]\n",
      "Training: loss = 0.30884, acc = 0.87050 |\n",
      "Epoch: 55, att_val: [1.]\n",
      "Training: loss = 0.30846, acc = 0.86091 |\n",
      "Epoch: 56, att_val: [1.]\n",
      "Training: loss = 0.30937, acc = 0.87050 |\n",
      "Epoch: 57, att_val: [1.]\n",
      "Training: loss = 0.31202, acc = 0.84892 |\n",
      "Epoch: 58, att_val: [1.]\n",
      "Training: loss = 0.32257, acc = 0.88249 |\n",
      "Epoch: 59, att_val: [1.]\n",
      "Training: loss = 0.33564, acc = 0.81295 |\n",
      "Epoch: 60, att_val: [1.]\n",
      "Training: loss = 0.35054, acc = 0.87530 |\n",
      "Epoch: 61, att_val: [1.]\n",
      "Training: loss = 0.31243, acc = 0.84892 |\n",
      "Epoch: 62, att_val: [1.]\n",
      "Training: loss = 0.31463, acc = 0.83693 |\n",
      "Epoch: 63, att_val: [1.]\n",
      "Training: loss = 0.35759, acc = 0.87770 |\n",
      "Epoch: 64, att_val: [1.]\n",
      "Training: loss = 0.32830, acc = 0.82494 |\n",
      "Epoch: 65, att_val: [1.]\n",
      "Training: loss = 0.30740, acc = 0.86331 |\n",
      "Epoch: 66, att_val: [1.]\n",
      "Training: loss = 0.32174, acc = 0.87770 |\n",
      "Epoch: 67, att_val: [1.]\n",
      "Training: loss = 0.30991, acc = 0.85612 |\n",
      "Epoch: 68, att_val: [1.]\n",
      "Training: loss = 0.31276, acc = 0.85372 |\n",
      "Epoch: 69, att_val: [1.]\n",
      "Training: loss = 0.32793, acc = 0.88010 |\n",
      "Epoch: 70, att_val: [1.]\n",
      "Training: loss = 0.31133, acc = 0.85612 |\n",
      "Epoch: 71, att_val: [1.]\n",
      "Training: loss = 0.31253, acc = 0.85132 |\n",
      "Epoch: 72, att_val: [1.]\n",
      "Training: loss = 0.31745, acc = 0.87290 |\n",
      "Epoch: 73, att_val: [1.]\n",
      "Training: loss = 0.30748, acc = 0.85851 |\n",
      "Epoch: 74, att_val: [1.]\n",
      "Training: loss = 0.31936, acc = 0.83933 |\n",
      "Epoch: 75, att_val: [1.]\n",
      "Training: loss = 0.32065, acc = 0.87770 |\n",
      "Epoch: 76, att_val: [1.]\n",
      "Training: loss = 0.30869, acc = 0.86331 |\n",
      "Epoch: 77, att_val: [1.]\n",
      "Training: loss = 0.32008, acc = 0.83693 |\n",
      "Epoch: 78, att_val: [1.]\n",
      "Training: loss = 0.31460, acc = 0.87290 |\n",
      "Epoch: 79, att_val: [1.]\n",
      "Training: loss = 0.31089, acc = 0.87050 |\n",
      "Epoch: 80, att_val: [1.]\n",
      "Training: loss = 0.32192, acc = 0.84173 |\n",
      "Epoch: 81, att_val: [1.]\n",
      "Training: loss = 0.31391, acc = 0.87290 |\n",
      "Epoch: 82, att_val: [1.]\n",
      "Training: loss = 0.31080, acc = 0.86571 |\n",
      "Epoch: 83, att_val: [1.]\n",
      "Training: loss = 0.31735, acc = 0.84173 |\n",
      "Epoch: 84, att_val: [1.]\n",
      "Training: loss = 0.30904, acc = 0.85851 |\n",
      "Epoch: 85, att_val: [1.]\n",
      "Training: loss = 0.31331, acc = 0.87770 |\n",
      "Epoch: 86, att_val: [1.]\n",
      "Training: loss = 0.31553, acc = 0.84652 |\n",
      "Epoch: 87, att_val: [1.]\n",
      "Training: loss = 0.30876, acc = 0.86091 |\n",
      "Epoch: 88, att_val: [1.]\n",
      "Training: loss = 0.31122, acc = 0.86811 |\n",
      "Epoch: 89, att_val: [1.]\n",
      "Training: loss = 0.31101, acc = 0.85851 |\n",
      "Epoch: 90, att_val: [1.]\n",
      "Training: loss = 0.30796, acc = 0.85851 |\n",
      "Epoch: 91, att_val: [1.]\n",
      "Training: loss = 0.31103, acc = 0.87770 |\n",
      "Epoch: 92, att_val: [1.]\n",
      "Training: loss = 0.30976, acc = 0.86331 |\n",
      "Epoch: 93, att_val: [1.]\n",
      "Training: loss = 0.30673, acc = 0.86331 |\n",
      "Epoch: 94, att_val: [1.]\n",
      "Training: loss = 0.30777, acc = 0.86091 |\n",
      "Epoch: 95, att_val: [1.]\n",
      "Training: loss = 0.30759, acc = 0.86091 |\n",
      "Epoch: 96, att_val: [1.]\n",
      "Training: loss = 0.30634, acc = 0.85612 |\n",
      "Epoch: 97, att_val: [1.]\n",
      "Training: loss = 0.30636, acc = 0.85851 |\n",
      "Epoch: 98, att_val: [1.]\n",
      "Training: loss = 0.30656, acc = 0.86331 |\n",
      "Epoch: 99, att_val: [1.]\n",
      "Training: loss = 0.30563, acc = 0.85851 |\n",
      "INFO:tensorflow:Restoring parameters from C:\\Backup of covid project\\2cls_negVSnonneg\\results\\acm_allMP_multi_fea_.ckpt\n",
      "load model from : C:\\Backup of covid project\\2cls_negVSnonneg\\results\\acm_allMP_multi_fea_.ckpt\n",
      "Test loss: 0.3014291226863861 ; Test accuracy: 0.8199999332427979\n",
      "start knn, kmean.....\n",
      "xx: (100, 32), yy: (100, 2)\n",
      "KNN(10avg, split:0.2, k=5) classification_rate: 0.8500, f1_micro: 0.8037\n",
      "KNN(10avg, split:0.4, k=5) classification_rate: 0.7833, f1_micro: 0.7800\n",
      "KNN(10avg, split:0.6, k=5) classification_rate: 0.7750, f1_micro: 0.7600\n",
      "KNN(10avg, split:0.8, k=5) classification_rate: 0.7000, f1_micro: 0.7550\n",
      "NMI (10 avg): 0.3033 , ARI (10avg): 0.1888\n"
     ]
    }
   ],
   "source": [
    "#Get the model\n",
    "#Use paths to determine which type of classification\n",
    "\n",
    "#path_result='C:\\\\Backup of covid project\\\\2cls_posVSnonpos\\\\results\\\\'\n",
    "#path_data=\"C:\\\\Backup of covid project\\\\2cls_posVSnonpos\\\\data\\\\\"\n",
    "#path_code=\"C:\\\\Backup of covid project\\\\\"\n",
    "\n",
    "path_result='C:\\\\Backup of covid project\\\\2cls_negVSnonneg\\\\results\\\\'\n",
    "path_data=\"C:\\\\Backup of covid project\\\\2cls_negVSnonneg\\\\data\\\\\"\n",
    "path_code=\"C:\\\\Backup of covid project\\\\\"\n",
    "\n",
    "#path_result='C:\\\\Backup of covid project\\\\2cls_wlNeu_woNeu\\\\results_WOneu\\\\'\n",
    "#path_data=\"C:\\\\Backup of covid project\\\\2cls_wlNeu_woNeu\\\\data\\\\\"\n",
    "#path_code=\"C:\\\\Backup of covid project\\\\\"\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import random\n",
    "os.chdir(path_code)\n",
    "import gat\n",
    "import imp\n",
    "imp.reload(gat)\n",
    "from gat import GAT, HeteGAT, HeteGAT_multi  # or * for that matter\n",
    "\n",
    "import process\n",
    "import importlib\n",
    "importlib.reload(process)\n",
    "# 禁用gpu\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2,3\"\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "dataset = 'acm'\n",
    "featype = 'fea'\n",
    "checkpt_file=path_result+'acm_allMP_multi_fea_.ckpt'\n",
    "#checkpt_file = 'C:\\data\\HAN-modified\\\\tranfer learning\\\\acm_allMP_multi_fea_2cls.ckpt'\n",
    "\n",
    "print('model: {}'.format(checkpt_file))\n",
    "# training params for only CC\n",
    "batch_size = 1\n",
    "nb_epochs = 100\n",
    "patience = 100\n",
    "lr = 0.02  # learning rate\n",
    "l2_coef = 0.08#0.1#0.001  # weight decay\n",
    "# numbers of hidden units per each attention head in each layer\n",
    "hid_units = [8]\n",
    "n_heads = [4, 1]  # additional entry for the output layer\n",
    "residual = False\n",
    "nonlinearity = tf.nn.elu\n",
    "model = HeteGAT_multi\n",
    "\n",
    "print('Dataset: ' + dataset)\n",
    "print('----- Opt. hyperparams -----')\n",
    "print('lr: ' + str(lr))\n",
    "print('l2_coef: ' + str(l2_coef))\n",
    "print('----- Archi. hyperparams -----')\n",
    "print('nb. layers: ' + str(len(hid_units)))\n",
    "print('nb. units per layer: ' + str(hid_units))\n",
    "print('nb. attention heads: ' + str(n_heads))\n",
    "print('residual: ' + str(residual))\n",
    "print('nonlinearity: ' + str(nonlinearity))\n",
    "print('model: ' + str(model))\n",
    "\n",
    "# jhy data\n",
    "import scipy.io as sio\n",
    "import scipy.sparse as sp\n",
    "\n",
    "\n",
    "def sample_mask(idx, l):\n",
    "    \"\"\"Create mask.\"\"\"\n",
    "    mask = np.zeros(l)\n",
    "    mask[idx] = 1\n",
    "    return np.array(mask, dtype=np.bool)\n",
    "\n",
    "def load_data_dblp(path=path_data):\n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "    import scipy.sparse\n",
    "    from scipy.sparse import csc_matrix\n",
    "    from scipy import sparse\n",
    "    import numpy as np \n",
    "    import os \n",
    "    import csv \n",
    "    import numpy as np\n",
    "    schools = [\"notredame\",\"uofm\",\"columbia\",\"dartmouth\",\"UCSD\",\"berkeley\",\"Harvard\",\"ucla\"]\n",
    "    s=schools[3]#use this index to control which network we are going to run.\n",
    "    t=\"comment\"\n",
    "    \n",
    "    os.chdir(path)\n",
    "    name=\"label_emt3_\"+s+\"_cm.csv\"\n",
    "    truelabels=pd.read_csv(name,skip_blank_lines=True,header=None)\n",
    "    name=\"feature_\"+s+t+\".csv\"\n",
    "    truefeatures=pd.read_csv(name,skip_blank_lines=True,header=None).values\n",
    "\n",
    "    N=truefeatures.shape[0]\n",
    "    name=\"CCsym_\"+s+\".npz\"\n",
    "    dat_cc=scipy.sparse.load_npz(name);dat_cc=dat_cc.toarray();\n",
    "    #name=\"CCasym_\"+s+\".npz\"\n",
    "    #dat_cca=scipy.sparse.load_npz(name);dat_cca=dat_cca.toarray();\n",
    "    name=\"CSCsy_\"+s+\".npz\"\n",
    "    dat_csc=scipy.sparse.load_npz(name);dat_csc=dat_csc.toarray();\n",
    "    name=\"CACsy_\"+s+\".npz\"\n",
    "    dat_cac=scipy.sparse.load_npz(name);dat_cac=dat_cac.toarray();\n",
    "    #rownetworks = [dat_csc,dat_cac]#,dat_cc]\n",
    "    #rownetworks = [np.transpose(dat_cc)]\n",
    "    rownetworks = [(dat_cc)]\n",
    "    #rownetworks = [np.zeros((N,N))]\n",
    "\n",
    "    y=truelabels\n",
    "    name=\"train_index_\"+s+\"_cm.p\"\n",
    "    train_idx = pickle.load(open(name,\"rb\"));train_idx=np.array(train_idx)\n",
    "    name=\"to_be_labeled_index_\"+s+\"_cm.p\"\n",
    "    val_idx = pickle.load(open(name,\"rb\"));val_idx=np.array(val_idx)\n",
    "    name=\"test_index_\"+s+\"_cm.p\"\n",
    "    test_idx = pickle.load(open(name,\"rb\"));test_idx=np.array(test_idx)\n",
    "\n",
    "    train_mask = sample_mask(train_idx, y.shape[0])\n",
    "    val_mask = sample_mask(val_idx, y.shape[0])\n",
    "    test_mask = sample_mask(test_idx, y.shape[0])\n",
    "    \n",
    "    y_train = np.zeros(y.shape)\n",
    "    y_val = np.zeros(y.shape)\n",
    "    y_test = np.zeros(y.shape)\n",
    "    y=y.values\n",
    "    y_train[train_mask, :] = y[train_mask, :]\n",
    "    y_val[val_mask, :] = y[val_mask, :]\n",
    "    y_test[test_mask, :] = y[test_mask, :]\n",
    "\n",
    "    # return selected_idx, selected_idx_2\n",
    "    print('y_train:{}, y_val:{}, y_test:{}, train_idx:{}, val_idx:{}, test_idx:{}'.format(y_train.shape,\n",
    "                                                                                          y_val.shape,\n",
    "                                                                                          y_test.shape,\n",
    "                                                                                          train_idx.shape,\n",
    "                                                                                          val_idx.shape,\n",
    "                                                                                          test_idx.shape))\n",
    "    truefeatures_list = [truefeatures, truefeatures]#, truefeatures, truefeatures]# ?? why copy three times? First for center node, 2 for each metapath.    \n",
    "    os.chdir(path_code)\n",
    "    return rownetworks, truefeatures_list, y_train, y_val, y_test, train_mask, val_mask, test_mask\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# use adj_list as fea_list, have a try~\n",
    "adj_list, fea_list, y_train, y_val, y_test, train_mask, val_mask, test_mask = load_data_dblp()\n",
    "if featype == 'adj':\n",
    "    fea_list = adj_list\n",
    "\n",
    "\n",
    "\n",
    "import scipy.sparse as sp\n",
    "\n",
    "nb_nodes = fea_list[0].shape[0]\n",
    "ft_size = fea_list[0].shape[1]\n",
    "nb_classes = y_train.shape[1]\n",
    "\n",
    "# features = features[np.newaxis]  # [1, nb_node, ft_size]\n",
    "fea_list = [fea[np.newaxis] for fea in fea_list]\n",
    "adj_list = [adj[np.newaxis] for adj in adj_list]\n",
    "y_train = y_train[np.newaxis]\n",
    "y_val = y_val[np.newaxis]\n",
    "y_test = y_test[np.newaxis]\n",
    "train_mask = train_mask[np.newaxis]\n",
    "val_mask = val_mask[np.newaxis]\n",
    "test_mask = test_mask[np.newaxis]\n",
    "\n",
    "biases_list = [process.adj_to_bias(adj, [nb_nodes], nhood=1) for adj in adj_list]\n",
    "#biases_list = [process.adj_to_bias(adj) for adj in adj_list]\n",
    "#process.adj_to_bias(adj)\n",
    "print('build graph...')\n",
    "with tf.Graph().as_default():\n",
    "    with tf.name_scope('input'):\n",
    "        ftr_in_list = [tf.placeholder(dtype=tf.float32,\n",
    "                                      shape=(batch_size, nb_nodes, ft_size),\n",
    "                                      name='ftr_in_{}'.format(i))\n",
    "                       for i in range(len(fea_list))]\n",
    "        bias_in_list = [tf.placeholder(dtype=tf.float32,\n",
    "                                       shape=(batch_size, nb_nodes, nb_nodes),\n",
    "                                       name='bias_in_{}'.format(i))\n",
    "                        for i in range(len(biases_list))]\n",
    "        lbl_in = tf.placeholder(dtype=tf.int32, shape=(\n",
    "            batch_size, nb_nodes, nb_classes), name='lbl_in')\n",
    "        msk_in = tf.placeholder(dtype=tf.int32, shape=(batch_size, nb_nodes),\n",
    "                                name='msk_in')\n",
    "        attn_drop = tf.placeholder(dtype=tf.float32, shape=(), name='attn_drop')\n",
    "        ffd_drop = tf.placeholder(dtype=tf.float32, shape=(), name='ffd_drop')\n",
    "        is_train = tf.placeholder(dtype=tf.bool, shape=(), name='is_train')\n",
    "    # forward\n",
    "    logits, final_embedding, att_val = model.inference(ftr_in_list, nb_classes, nb_nodes, is_train,\n",
    "                                                       attn_drop, ffd_drop,\n",
    "                                                       bias_mat_list=bias_in_list,\n",
    "                                                       hid_units=hid_units, n_heads=n_heads,\n",
    "                                                       residual=residual, activation=nonlinearity)\n",
    "\n",
    "    # cal masked_loss\n",
    "    log_resh = tf.reshape(logits, [-1, nb_classes])\n",
    "    lab_resh = tf.reshape(lbl_in, [-1, nb_classes])\n",
    "    msk_resh = tf.reshape(msk_in, [-1])\n",
    "    loss = model.masked_softmax_cross_entropy(log_resh, lab_resh, msk_resh)\n",
    "    predicted=tf.argmax(log_resh, 1)\n",
    "    accuracy = model.masked_accuracy(log_resh, lab_resh, msk_resh)\n",
    "    # optimzie\n",
    "    train_op = model.training(loss, lr, l2_coef)\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    init_op = tf.group(tf.global_variables_initializer(),\n",
    "                       tf.local_variables_initializer())\n",
    "\n",
    "    vlss_mn = np.inf\n",
    "    vacc_mx = 0.0\n",
    "    curr_step = 0\n",
    "\n",
    "    with tf.Session(config=config) as sess:\n",
    "        sess.run(init_op)\n",
    "\n",
    "        train_loss_avg = 0\n",
    "        train_acc_avg = 0\n",
    "        val_loss_avg = 0\n",
    "        val_acc_avg = 0\n",
    "        val_pred_avg = 0\n",
    "        for epoch in range(nb_epochs):\n",
    "            tr_step = 0\n",
    "           \n",
    "            tr_size = fea_list[0].shape[0]\n",
    "            # ================   training    ============\n",
    "            while tr_step * batch_size < tr_size:\n",
    "\n",
    "                fd1 = {i: d[tr_step * batch_size:(tr_step + 1) * batch_size]\n",
    "                       for i, d in zip(ftr_in_list, fea_list)}\n",
    "                fd2 = {i: d[tr_step * batch_size:(tr_step + 1) * batch_size]\n",
    "                       for i, d in zip(bias_in_list, biases_list)}\n",
    "                fd3 = {lbl_in: y_train[tr_step * batch_size:(tr_step + 1) * batch_size],\n",
    "                       msk_in: train_mask[tr_step * batch_size:(tr_step + 1) * batch_size],\n",
    "                       is_train: True,\n",
    "                       attn_drop: 0,\n",
    "                       ffd_drop: 0}\n",
    "                fd = fd1\n",
    "                fd.update(fd2)\n",
    "                fd.update(fd3)\n",
    "                _, loss_value_tr, acc_tr, att_val_train = sess.run([train_op, loss, accuracy, att_val],\n",
    "                                                                   feed_dict=fd)\n",
    "                train_loss_avg += loss_value_tr\n",
    "                train_acc_avg += acc_tr\n",
    "                tr_step += 1\n",
    "\n",
    "            vl_step = 0\n",
    "            vl_size = fea_list[0].shape[0]\n",
    "\n",
    "            # import pdb; pdb.set_trace()\n",
    "            print('Epoch: {}, att_val: {}'.format(epoch, np.mean(att_val_train, axis=0)))\n",
    "            print('Training: loss = %.5f, acc = %.5f |' %\n",
    "                  (train_loss_avg / tr_step, train_acc_avg / tr_step))\n",
    "\n",
    "            curr_step += 1\n",
    "            if epoch==nb_epochs-1:\n",
    "                saver.save(sess, checkpt_file)\n",
    "\n",
    "            train_loss_avg = 0\n",
    "            train_acc_avg = 0\n",
    "            val_loss_avg = 0\n",
    "            val_acc_avg = 0\n",
    "        #save_path = saver.save(sess, \"C:\\data\\HAN-modified\\\\tranfer learning\\model.ckpt\")\n",
    "        #print(\"Model saved in path: %s\" % save_path)\n",
    "        saver.restore(sess, checkpt_file)\n",
    "        print('load model from : {}'.format(checkpt_file))\n",
    "        ts_size = fea_list[0].shape[0]\n",
    "        ts_step = 0\n",
    "        ts_loss = 0.0\n",
    "        ts_acc = 0.0\n",
    "\n",
    "        while ts_step * batch_size < ts_size:\n",
    "            # fd1 = {ftr_in: features[ts_step * batch_size:(ts_step + 1) * batch_size]}\n",
    "            fd1 = {i: d[ts_step * batch_size:(ts_step + 1) * batch_size]\n",
    "                   for i, d in zip(ftr_in_list, fea_list)}\n",
    "            fd2 = {i: d[ts_step * batch_size:(ts_step + 1) * batch_size]\n",
    "                   for i, d in zip(bias_in_list, biases_list)}\n",
    "            fd3 = {lbl_in: y_test[ts_step * batch_size:(ts_step + 1) * batch_size],\n",
    "                   msk_in: test_mask[ts_step * batch_size:(ts_step + 1) * batch_size],\n",
    "            \n",
    "                   is_train: False,\n",
    "                   attn_drop: 0.0,\n",
    "                   ffd_drop: 0.0}\n",
    "        \n",
    "            fd = fd1\n",
    "            fd.update(fd2)\n",
    "            fd.update(fd3)\n",
    "            loss_value_ts, acc_ts, jhy_final_embedding = sess.run([loss, accuracy, final_embedding],\n",
    "                                                                  feed_dict=fd)\n",
    "            ts_loss += loss_value_ts\n",
    "            ts_acc += acc_ts\n",
    "            ts_step += 1\n",
    "\n",
    "        print('Test loss:', ts_loss / ts_step,\n",
    "              '; Test accuracy:', ts_acc / ts_step)\n",
    "        \n",
    "        import pickle\n",
    "        \n",
    "        print('start knn, kmean.....')\n",
    "        #os.chdir('C:\\\\data\\\\HAN-modified\\\\')\n",
    "        xx = np.expand_dims(jhy_final_embedding, axis=0)[test_mask]\n",
    "\n",
    "        from numpy import linalg as LA\n",
    "\n",
    "        # xx = xx / LA.norm(xx, axis=1)\n",
    "        yy = y_test[test_mask]\n",
    "\n",
    "        print('xx: {}, yy: {}'.format(xx.shape, yy.shape))\n",
    "        from jhyexp import my_KNN, my_Kmeans#, my_TSNE, my_Linear\n",
    "\n",
    "        my_KNN(xx, yy)\n",
    "        my_Kmeans(xx, yy)\n",
    "\n",
    "        sess.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: acm\n",
      "----- Opt. hyperparams -----\n",
      "100\n",
      "y_train:(1308, 2), y_val:(1308, 2), y_test:(1308, 2), train_idx:(80,), val_idx:(1128,), test_idx:(1308,)\n",
      "s\n",
      "build graph...\n",
      "de\n",
      "INFO:tensorflow:Restoring parameters from C:\\Backup of covid project\\2cls_negVSnonneg\\results\\acm_allMP_multi_fea_.ckpt\n",
      "Model restored.\n",
      "(1, 1308, 768)\n",
      "test start\n",
      "0\n",
      "1\n",
      "1\n",
      "Test loss: 0.10332329571247101 ; Test accuracy: 0.46712538599967957\n",
      "100\n",
      "y_train:(7064, 2), y_val:(7064, 2), y_test:(7064, 2), train_idx:(106,), val_idx:(6858,), test_idx:(7064,)\n",
      "s\n",
      "build graph...\n",
      "de\n",
      "INFO:tensorflow:Restoring parameters from C:\\Backup of covid project\\2cls_negVSnonneg\\results\\acm_allMP_multi_fea_.ckpt\n",
      "Model restored.\n",
      "(1, 7064, 768)\n",
      "test start\n",
      "0\n",
      "1\n",
      "1\n",
      "Test loss: 0.02360055409371853 ; Test accuracy: 0.672706663608551\n",
      "100\n",
      "y_train:(10362, 2), y_val:(10362, 2), y_test:(10362, 2), train_idx:(83,), val_idx:(10179,), test_idx:(10362,)\n",
      "s\n",
      "build graph...\n",
      "de\n",
      "INFO:tensorflow:Restoring parameters from C:\\Backup of covid project\\2cls_negVSnonneg\\results\\acm_allMP_multi_fea_.ckpt\n",
      "Model restored.\n",
      "(1, 10362, 768)\n",
      "test start\n",
      "0\n",
      "1\n",
      "1\n",
      "Test loss: 0.01513654738664627 ; Test accuracy: 0.47876858711242676\n",
      "100\n",
      "y_train:(11619, 2), y_val:(11619, 2), y_test:(11619, 2), train_idx:(70,), val_idx:(11449,), test_idx:(11619,)\n",
      "s\n",
      "build graph...\n",
      "de\n",
      "INFO:tensorflow:Restoring parameters from C:\\Backup of covid project\\2cls_negVSnonneg\\results\\acm_allMP_multi_fea_.ckpt\n",
      "Model restored.\n",
      "(1, 11619, 768)\n",
      "test start\n",
      "0\n",
      "1\n",
      "1\n",
      "Test loss: 0.011819089762866497 ; Test accuracy: 0.6001377105712891\n",
      "100\n",
      "y_train:(4248, 2), y_val:(4248, 2), y_test:(4248, 2), train_idx:(44,), val_idx:(4104,), test_idx:(4248,)\n",
      "s\n",
      "build graph...\n",
      "de\n",
      "INFO:tensorflow:Restoring parameters from C:\\Backup of covid project\\2cls_negVSnonneg\\results\\acm_allMP_multi_fea_.ckpt\n",
      "Model restored.\n",
      "(1, 4248, 768)\n",
      "test start\n",
      "0\n",
      "1\n",
      "1\n",
      "Test loss: 0.0264566820114851 ; Test accuracy: 0.4929378628730774\n",
      "100\n",
      "y_train:(7693, 2), y_val:(7693, 2), y_test:(7693, 2), train_idx:(40,), val_idx:(7553,), test_idx:(7693,)\n",
      "s\n",
      "build graph...\n",
      "de\n",
      "INFO:tensorflow:Restoring parameters from C:\\Backup of covid project\\2cls_negVSnonneg\\results\\acm_allMP_multi_fea_.ckpt\n",
      "Model restored.\n",
      "(1, 7693, 768)\n",
      "test start\n",
      "0\n",
      "1\n",
      "1\n",
      "Test loss: 0.012953359633684158 ; Test accuracy: 0.5178734064102173\n",
      "100\n",
      "y_train:(2009, 2), y_val:(2009, 2), y_test:(2009, 2), train_idx:(417,), val_idx:(1492,), test_idx:(2009,)\n",
      "s\n",
      "build graph...\n",
      "de\n",
      "INFO:tensorflow:Restoring parameters from C:\\Backup of covid project\\2cls_negVSnonneg\\results\\acm_allMP_multi_fea_.ckpt\n",
      "Model restored.\n",
      "(1, 2009, 768)\n",
      "test start\n",
      "0\n",
      "1\n",
      "1\n",
      "Test loss: 0.21815288066864014 ; Test accuracy: 0.5241413712501526\n",
      "100\n",
      "y_train:(12066, 2), y_val:(12066, 2), y_test:(12066, 2), train_idx:(48,), val_idx:(11918,), test_idx:(12066,)\n",
      "s\n",
      "build graph...\n",
      "de\n",
      "INFO:tensorflow:Restoring parameters from C:\\Backup of covid project\\2cls_negVSnonneg\\results\\acm_allMP_multi_fea_.ckpt\n",
      "Model restored.\n",
      "(1, 12066, 768)\n",
      "test start\n",
      "0\n",
      "1\n",
      "1\n",
      "Test loss: 0.009125855751335621 ; Test accuracy: 0.5098624229431152\n",
      "100\n",
      "y_train:(11756, 2), y_val:(11756, 2), y_test:(11756, 2), train_idx:(51,), val_idx:(11605,), test_idx:(11756,)\n",
      "s\n",
      "build graph...\n",
      "de\n",
      "INFO:tensorflow:Restoring parameters from C:\\Backup of covid project\\2cls_negVSnonneg\\results\\acm_allMP_multi_fea_.ckpt\n",
      "Model restored.\n",
      "(1, 11756, 768)\n",
      "test start\n",
      "0\n",
      "1\n",
      "1\n",
      "Test loss: 0.011749251745641232 ; Test accuracy: 0.5177781581878662\n",
      "100\n",
      "y_train:(12083, 2), y_val:(12083, 2), y_test:(12083, 2), train_idx:(47,), val_idx:(11936,), test_idx:(12083,)\n",
      "s\n",
      "build graph...\n",
      "de\n",
      "INFO:tensorflow:Restoring parameters from C:\\Backup of covid project\\2cls_negVSnonneg\\results\\acm_allMP_multi_fea_.ckpt\n",
      "Model restored.\n",
      "(1, 12083, 768)\n",
      "test start\n",
      "0\n",
      "1\n",
      "1\n",
      "Test loss: 0.013243449851870537 ; Test accuracy: 0.5315732955932617\n",
      "100\n",
      "y_train:(11704, 2), y_val:(11704, 2), y_test:(11704, 2), train_idx:(32,), val_idx:(11572,), test_idx:(11704,)\n",
      "s\n",
      "build graph...\n",
      "de\n",
      "INFO:tensorflow:Restoring parameters from C:\\Backup of covid project\\2cls_negVSnonneg\\results\\acm_allMP_multi_fea_.ckpt\n",
      "Model restored.\n",
      "(1, 11704, 768)\n",
      "test start\n",
      "0\n",
      "1\n",
      "1\n",
      "Test loss: 0.014510984532535076 ; Test accuracy: 0.5399863123893738\n",
      "100\n",
      "y_train:(2366, 2), y_val:(2366, 2), y_test:(2366, 2), train_idx:(48,), val_idx:(2218,), test_idx:(2366,)\n",
      "s\n",
      "build graph...\n",
      "de\n",
      "INFO:tensorflow:Restoring parameters from C:\\Backup of covid project\\2cls_negVSnonneg\\results\\acm_allMP_multi_fea_.ckpt\n",
      "Model restored.\n",
      "(1, 2366, 768)\n",
      "test start\n",
      "0\n",
      "1\n",
      "1\n",
      "Test loss: 0.05953032895922661 ; Test accuracy: 0.5109890103340149\n",
      "100\n",
      "y_train:(2924, 2), y_val:(2924, 2), y_test:(2924, 2), train_idx:(73,), val_idx:(2751,), test_idx:(2924,)\n",
      "s\n",
      "build graph...\n",
      "de\n",
      "INFO:tensorflow:Restoring parameters from C:\\Backup of covid project\\2cls_negVSnonneg\\results\\acm_allMP_multi_fea_.ckpt\n",
      "Model restored.\n",
      "(1, 2924, 768)\n",
      "test start\n",
      "0\n",
      "1\n",
      "1\n",
      "Test loss: 0.0516740083694458 ; Test accuracy: 0.5420656800270081\n",
      "100\n",
      "y_train:(11910, 2), y_val:(11910, 2), y_test:(11910, 2), train_idx:(39,), val_idx:(11771,), test_idx:(11910,)\n",
      "s\n",
      "build graph...\n",
      "de\n",
      "INFO:tensorflow:Restoring parameters from C:\\Backup of covid project\\2cls_negVSnonneg\\results\\acm_allMP_multi_fea_.ckpt\n",
      "Model restored.\n",
      "(1, 11910, 768)\n",
      "test start\n",
      "0\n",
      "1\n",
      "1\n",
      "Test loss: 0.012114740908145905 ; Test accuracy: 0.49832072854042053\n",
      "100\n",
      "y_train:(11626, 2), y_val:(11626, 2), y_test:(11626, 2), train_idx:(34,), val_idx:(11492,), test_idx:(11626,)\n",
      "s\n",
      "build graph...\n",
      "de\n",
      "INFO:tensorflow:Restoring parameters from C:\\Backup of covid project\\2cls_negVSnonneg\\results\\acm_allMP_multi_fea_.ckpt\n",
      "Model restored.\n",
      "(1, 11626, 768)\n",
      "test start\n",
      "0\n",
      "1\n",
      "1\n",
      "Test loss: 0.013699102215468884 ; Test accuracy: 0.5492860674858093\n"
     ]
    }
   ],
   "source": [
    "#Use model from above running to get predictions for all datasets\n",
    "#the model used and type of classification depends on paths from previous block\n",
    "#don't run this without running the previous one\n",
    "#The print-outs in this block is useless and meaningless, \n",
    "#the correct prediction for each sample has been saved to paths specified in previous block\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "os.chdir(path_code)\n",
    "from gat import GAT, HeteGAT, HeteGAT_multi\n",
    "import process\n",
    "import importlib\n",
    "from layers import attn_head, SimpleAttLayer\n",
    "importlib.reload(process)\n",
    "# 禁用gpu\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2,3\"\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "dataset = 'acm'\n",
    "featype = 'fea'\n",
    "# training params\n",
    "\n",
    "\n",
    "\n",
    "print('Dataset: ' + dataset)\n",
    "print('----- Opt. hyperparams -----')\n",
    "#print('lr: ' + str(lr))\n",
    "#print('l2_coef: ' + str(l2_coef))\n",
    "#print('----- Archi. hyperparams -----')\n",
    "#print('nb. layers: ' + str(len(hid_units)))\n",
    "#print('nb. units per layer: ' + str(hid_units))\n",
    "#print('nb. attention heads: ' + str(n_heads))\n",
    "#print('residual: ' + str(residual))\n",
    "#print('nonlinearity: ' + str(nonlinearity))\n",
    "#print('model: ' + str(model))\n",
    "\n",
    "# jhy data\n",
    "import scipy.io as sio\n",
    "import scipy.sparse as sp\n",
    "\n",
    "\n",
    "def sample_mask(idx, l):\n",
    "    \"\"\"Create mask.\"\"\"\n",
    "    mask = np.zeros(l)\n",
    "    mask[idx] = 1\n",
    "    return np.array(mask, dtype=np.bool)\n",
    "\n",
    "#path_cur='C:\\\\data\\\\HAN-modified\\\\tranfer learning\\\\'\n",
    "#path_data='C:\\\\data\\\\HAN-modified\\\\data\\\\preprocessed-trial\\\\'\n",
    "\n",
    "schools = [\"notredame2019\",\"notredame2020\",\"uofm2019\",\"uofm2020\",\"columbia2019\",\"columbia2020\",\"dartmouth\",\"UCSD2019\",\"UCSD2020\",\"berkeley2019\",\"berkeley2020\",\"Harvard2019\",\"Harvard2020\",\"ucla2019\",\"ucla2020\"]\n",
    "schools_type = [\"notredamecomment2019\",\"notredamecomment2020\",\"uofmcomment2019\",\"uofmcomment2020\",\"columbiacomment2019\",\"columbiacomment2020\",\"dartmouthcomment\",\"UCSDcomment2019\",\"UCSDcomment2020\",\n",
    "           \"berkeleycomment2019\",\"berkeleycomment2020\",\"Harvardcomment2019\",\"Harvardcomment2020\",\"uclacomment2019\",\"uclacomment2020\"]\n",
    "\n",
    "\n",
    "#path_data='C:\\\\data\\\\HAN-modified\\\\tranfer learning\\\\'\n",
    "def load_data_dblp(s,st,path=path_data):\n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "    import scipy.sparse\n",
    "    from scipy.sparse import csc_matrix\n",
    "    from scipy import sparse\n",
    "    import numpy as np \n",
    "    import os \n",
    "    import csv \n",
    "    import numpy as np\n",
    "    #schools = [\"notredame\",\"uofm\",\"columbia\",\"dartmouth\",\"UCSD\",\"berkeley\",\"Harvard\",\"ucla\"]\n",
    "    #s=schools[3]#use this index to control which network we are going to run.\n",
    "    t=\"comment\"\n",
    "    \n",
    "    os.chdir(path)\n",
    "    name=\"label_emt3_\"+s+\"_cm.csv\"\n",
    "    truelabels=pd.read_csv(name,skip_blank_lines=True,header=None)\n",
    "    name=\"feature_\"+st+\".csv\"\n",
    "    truefeatures=pd.read_csv(name,skip_blank_lines=True,header=None).values\n",
    "    #name=\"feature_\"+s+t+\".npz\"\n",
    "    #truefeatures=scipy.sparse.load_npz(name)\n",
    "    #truefeatures=truefeatures.toarray()\n",
    "\n",
    "    N=truefeatures.shape[0]\n",
    "    name=\"CCsym_\"+s+\".npz\"\n",
    "    dat_cc=scipy.sparse.load_npz(name);dat_cc=dat_cc.toarray();\n",
    "    #name=\"CCasym_\"+s+\".npz\"\n",
    "    #dat_cca=scipy.sparse.load_npz(name);dat_cca=dat_cca.toarray();\n",
    "    name=\"CSCsy_\"+s+\".npz\"\n",
    "    dat_csc=scipy.sparse.load_npz(name);dat_csc=dat_csc.toarray();\n",
    "    name=\"CACsy_\"+s+\".npz\"\n",
    "    dat_cac=scipy.sparse.load_npz(name);dat_cac=dat_cac.toarray();\n",
    "    #rownetworks = [dat_csc,dat_cac]#,dat_cc]\n",
    "    #rownetworks = [np.transpose(dat_cc)]\n",
    "    rownetworks = [(dat_cc)]\n",
    "    #rownetworks = [dat_cc,dat_csc]#,dat_cac]\n",
    "    #rownetworks = [dat_cc,dat_cac]#,dat_csc]\n",
    "    #rownetworks = [np.zeros((N,N))]\n",
    "\n",
    "    y=truelabels\n",
    "    name=\"train_index_\"+s+\"_cm.p\"\n",
    "    train_idx = pickle.load(open(name,\"rb\"));train_idx=np.array(train_idx)\n",
    "    name=\"to_be_labeled_index_\"+s+\"_cm.p\"\n",
    "    val_idx = pickle.load(open(name,\"rb\"));val_idx=np.array(val_idx)\n",
    "    name=\"test_index_\"+s+\"_cm.p\"\n",
    "    test_idx = pickle.load(open(name,\"rb\"));test_idx=np.array(test_idx)\n",
    "    print(len(test_idx))\n",
    "    #use this line to control testing set\n",
    "    test_idx=np.array(range(N))\n",
    "    \n",
    "    train_mask = sample_mask(train_idx, y.shape[0])\n",
    "    val_mask = sample_mask(val_idx, y.shape[0])\n",
    "    test_mask = sample_mask(test_idx, y.shape[0])# ?? why copy three times? First for center node, 2 for each metapath.\n",
    "    \n",
    "    y_train = np.zeros(y.shape)\n",
    "    y_val = np.zeros(y.shape)\n",
    "    y_test = np.zeros(y.shape)\n",
    "    y=y.values\n",
    "    y_train[train_mask, :] = y[train_mask, :]\n",
    "    y_val[val_mask, :] = y[val_mask, :]\n",
    "    y_test[test_mask, :] = y[test_mask, :]\n",
    "\n",
    "    # return selected_idx, selected_idx_2\n",
    "    print('y_train:{}, y_val:{}, y_test:{}, train_idx:{}, val_idx:{}, test_idx:{}'.format(y_train.shape,\n",
    "                                                                                          y_val.shape,\n",
    "                                                                                          y_test.shape,\n",
    "                                                                                          train_idx.shape,\n",
    "                                                                                          val_idx.shape,\n",
    "                                                                                          test_idx.shape))\n",
    "    truefeatures_list = [truefeatures, truefeatures]#, truefeatures, truefeatures]# ?? why copy three times? First for center node, 2 for each metapath.    \n",
    "    os.chdir(path_code)\n",
    "    return rownetworks, truefeatures_list, y_train, y_val, y_test, train_mask, val_mask, test_mask\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(schools)):\n",
    "    adj_list, fea_list, y_train, y_val, y_test, train_mask, val_mask, test_mask = load_data_dblp(s=schools[i],st=schools_type[i])\n",
    "    \n",
    "\n",
    "    if featype == 'adj':\n",
    "        fea_list = adj_list\n",
    "\n",
    "\n",
    "\n",
    "    import scipy.sparse as sp\n",
    "\n",
    "\n",
    "\n",
    "    nb_nodes = fea_list[0].shape[0]\n",
    "    ft_size = fea_list[0].shape[1]\n",
    "    nb_classes = y_train.shape[1]\n",
    "\n",
    "    # adj = adj.todense()\n",
    "    \n",
    "    # features = features[np.newaxis]  # [1, nb_node, ft_size]\n",
    "    fea_list = [fea[np.newaxis] for fea in fea_list]\n",
    "    adj_list = [adj[np.newaxis] for adj in adj_list]\n",
    "    y_train = y_train[np.newaxis]\n",
    "    y_val = y_val[np.newaxis]\n",
    "    y_test = y_test[np.newaxis]\n",
    "    train_mask = train_mask[np.newaxis]\n",
    "    val_mask = val_mask[np.newaxis]\n",
    "    test_mask = test_mask[np.newaxis]\n",
    "    print(\"s\")\n",
    "    #biases_list = [process.adj_to_bias(adj, [nb_nodes], nhood=1) for adj in adj_list\n",
    "    #biases_list = [process.adj_to_bias(adj.astype('float') ) for adj in adj_list]\n",
    "    \n",
    "    #biases_list = [process.adj_to_bias(adj, [nb_nodes], nhood=1) for adj in adj_list]\n",
    "    import pickle\n",
    "    name=schools[i]+'adj.p'\n",
    "    os.chdir(path_data)\n",
    "    #pickle.dump(biases_list,open(name,\"wb\"))\n",
    "    biases_list=pickle.load(open(name,\"rb\"))\n",
    "    \n",
    "    #For two year separate\n",
    "    print('build graph...')\n",
    "    #checkpt_file = 'C:\\data\\HAN-modified\\\\tranfer learning\\model.ckpt'\n",
    "    with tf.Graph().as_default():\n",
    "        with tf.name_scope('input'):\n",
    "            ftr_in_list = [tf.placeholder(dtype=tf.float32,\n",
    "                                          shape=(batch_size, nb_nodes, ft_size),\n",
    "                                          name='ftr_in_{}'.format(i))\n",
    "                           for i in range(len(fea_list))]\n",
    "            bias_in_list = [tf.placeholder(dtype=tf.float32,\n",
    "                                           shape=(batch_size, nb_nodes, nb_nodes),\n",
    "                                           name='bias_in_{}'.format(i))\n",
    "                            for i in range(len(biases_list))]\n",
    "            lbl_in = tf.placeholder(dtype=tf.int32, shape=(\n",
    "                batch_size, nb_nodes, nb_classes), name='lbl_in')\n",
    "            msk_in = tf.placeholder(dtype=tf.int32, shape=(batch_size, nb_nodes),\n",
    "                                    name='msk_in')\n",
    "            attn_drop = tf.placeholder(dtype=tf.float32, shape=(), name='attn_drop')\n",
    "            ffd_drop = tf.placeholder(dtype=tf.float32, shape=(), name='ffd_drop')\n",
    "            is_train = tf.placeholder(dtype=tf.bool, shape=(), name='is_train')\n",
    "        # forward\n",
    "        logits, final_embedding, att_val = model.inference(ftr_in_list, nb_classes, nb_nodes, is_train,attn_drop, ffd_drop,bias_mat_list=bias_in_list,hid_units=hid_units, n_heads=n_heads,residual=residual, activation=nonlinearity)\n",
    "    \n",
    "        # cal masked_loss\n",
    "        log_resh = tf.reshape(logits, [-1, nb_classes])\n",
    "        lab_resh = tf.reshape(lbl_in, [-1, nb_classes])\n",
    "        msk_resh = tf.reshape(msk_in, [-1])\n",
    "        loss = model.masked_softmax_cross_entropy(log_resh, lab_resh, msk_resh)\n",
    "        accuracy = model.masked_accuracy(log_resh, lab_resh, msk_resh)\n",
    "        # optimzie\n",
    "        train_op = model.training(loss, lr, l2_coef)\n",
    "        vlss_mn = np.inf\n",
    "        vacc_mx = 0.0\n",
    "        curr_step = 0\n",
    "        saver = tf.train.Saver()\n",
    "        init_op = tf.group(tf.global_variables_initializer(),\n",
    "                           tf.local_variables_initializer())\n",
    "        #new_saver = tf.train.import_meta_graph('C:\\data\\HAN-modified\\\\tranfer learning\\model.ckpt.meta')\n",
    "    \n",
    "    \n",
    "        with tf.Session(config=config) as sess:\n",
    "            #sess.run(init_op)\n",
    "\n",
    "            train_loss_avg = 0\n",
    "            train_acc_avg = 0\n",
    "            val_loss_avg = 0\n",
    "            val_acc_avg = 0\n",
    "            os.chdir(path_code)\n",
    "            saver.restore(sess, path_result+\"acm_allMP_multi_fea_.ckpt\")\n",
    "            #saver.restore(sess, \"C:\\data\\HAN-modified\\\\tranfer learning\\\\model.ckpt\")\n",
    "            #new_saver.restore(sess, tf.train.latest_checkpoint('./'))\n",
    "            print(\"Model restored.\")\n",
    "            \n",
    "            #saver.restore(sess, checkpt_file)\n",
    "            #print('load model from : {}'.format(checkpt_file))\n",
    "            ts_size = fea_list[0].shape[0]\n",
    "            print(fea_list[0].shape)\n",
    "            ts_step = 0\n",
    "            ts_loss = 0.0\n",
    "            ts_acc = 0.0\n",
    "            print(\"test start\")\n",
    "            while ts_step * batch_size < ts_size:\n",
    "                print(ts_step)\n",
    "                print(batch_size)\n",
    "                print(ts_size)\n",
    "                # fd1 = {ftr_in: features[ts_step * batch_size:(ts_step + 1) * batch_size]}\n",
    "                fd1 = {i: d[ts_step * batch_size:(ts_step + 1) * batch_size]\n",
    "                       for i, d in zip(ftr_in_list, fea_list)}\n",
    "                fd2 = {i: d[ts_step * batch_size:(ts_step + 1) * batch_size]\n",
    "                       for i, d in zip(bias_in_list, biases_list)}\n",
    "                fd3 = {lbl_in: y_test[ts_step * batch_size:(ts_step + 1) * batch_size],\n",
    "                       msk_in: test_mask[ts_step * batch_size:(ts_step + 1) * batch_size],\n",
    "                \n",
    "                       is_train: False,\n",
    "                       attn_drop: 0.0,\n",
    "                       ffd_drop: 0.0}\n",
    "            \n",
    "                fd = fd1\n",
    "                fd.update(fd2)\n",
    "                fd.update(fd3)\n",
    "                \n",
    "                loss_value_ts, acc_ts, jhy_final_embedding, predicted = sess.run([loss, accuracy, final_embedding, log_resh],\n",
    "                                                                      feed_dict=fd)\n",
    "                import pickle\n",
    "                name=schools[i]+'.p'\n",
    "                os.chdir(path_result)\n",
    "                pickle.dump(predicted,open(name,\"wb\"))\n",
    "                ts_loss += loss_value_ts\n",
    "                ts_acc += acc_ts\n",
    "                ts_step += 1\n",
    "\n",
    "            print('Test loss:', ts_loss / ts_step,\n",
    "                  '; Test accuracy:', ts_acc / ts_step)\n",
    "    \n",
    "            \n",
    "            sess.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
