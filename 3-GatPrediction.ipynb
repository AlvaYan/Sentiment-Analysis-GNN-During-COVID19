{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dense-appeal",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the model\n",
    "path_result='C:\\\\Backup of covid project\\\\2cls_negVSnonneg\\\\results\\\\'\n",
    "path_data=\"C:\\\\Backup of covid project\\\\2cls_negVSnonneg\\\\data\\\\\"\n",
    "path_code=\"C:\\\\Backup of covid project\\\\\"\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import random\n",
    "os.chdir(path_code)\n",
    "import gat\n",
    "import imp\n",
    "imp.reload(gat)\n",
    "from gat import GAT, HeteGAT, HeteGAT_multi  # or * for that matter\n",
    "import process\n",
    "import importlib\n",
    "importlib.reload(process)\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import scipy.sparse\n",
    "from scipy.sparse import csc_matrix\n",
    "from scipy import sparse\n",
    "import csv \n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2,3\"\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "dataset = 'acm'\n",
    "featype = 'fea'\n",
    "checkpt_file=path_result+'acm_allMP_multi_fea_.ckpt'\n",
    "\n",
    "print('model: {}'.format(checkpt_file))\n",
    "# training params for only CC\n",
    "batch_size = 1\n",
    "nb_epochs = 100\n",
    "patience = 100\n",
    "lr = 0.02  # learning rate\n",
    "l2_coef = 0.08#0.1#0.001  # weight decay\n",
    "# numbers of hidden units per each attention head in each layer\n",
    "hid_units = [8]\n",
    "n_heads = [4, 1]  # additional entry for the output layer\n",
    "residual = False\n",
    "nonlinearity = tf.nn.elu\n",
    "model = HeteGAT_multi\n",
    "\n",
    "print('Dataset: ' + dataset)\n",
    "print('----- Opt. hyperparams -----')\n",
    "print('lr: ' + str(lr))\n",
    "print('l2_coef: ' + str(l2_coef))\n",
    "print('----- Archi. hyperparams -----')\n",
    "print('nb. layers: ' + str(len(hid_units)))\n",
    "print('nb. units per layer: ' + str(hid_units))\n",
    "print('nb. attention heads: ' + str(n_heads))\n",
    "print('residual: ' + str(residual))\n",
    "print('nonlinearity: ' + str(nonlinearity))\n",
    "print('model: ' + str(model))\n",
    "\n",
    "# jhy data\n",
    "import scipy.io as sio\n",
    "import scipy.sparse as sp\n",
    "\n",
    "\n",
    "def sample_mask(idx, l):\n",
    "    \"\"\"Create mask.\"\"\"\n",
    "    mask = np.zeros(l)\n",
    "    mask[idx] = 1\n",
    "    return np.array(mask, dtype=np.bool)\n",
    "\n",
    "def load_data_dblp(path=path_data):\n",
    "    schools = [\"notredame\",\"uofm\",\"columbia\",\"dartmouth\",\"UCSD\",\"berkeley\",\"Harvard\",\"ucla\"]\n",
    "    s=schools[3]#use this index to control which network we are going to run.\n",
    "    t=\"comment\"\n",
    "    \n",
    "    os.chdir(path)\n",
    "    name=\"label_emt3_\"+s+\"_cm.csv\"\n",
    "    truelabels=pd.read_csv(name,skip_blank_lines=True,header=None)\n",
    "    name=\"feature_\"+s+t+\".csv\"\n",
    "    truefeatures=pd.read_csv(name,skip_blank_lines=True,header=None).values\n",
    "\n",
    "    N=truefeatures.shape[0]\n",
    "    name=\"CCsym_\"+s+\".npz\"\n",
    "    dat_cc=scipy.sparse.load_npz(name);dat_cc=dat_cc.toarray();\n",
    "    rownetworks = [(dat_cc)]\n",
    "\n",
    "    y=truelabels\n",
    "    name=\"train_index_\"+s+\"_cm.p\"\n",
    "    train_idx = pickle.load(open(name,\"rb\"));train_idx=np.array(train_idx)\n",
    "    name=\"to_be_labeled_index_\"+s+\"_cm.p\"\n",
    "    val_idx = pickle.load(open(name,\"rb\"));val_idx=np.array(val_idx)\n",
    "    name=\"test_index_\"+s+\"_cm.p\"\n",
    "    test_idx = pickle.load(open(name,\"rb\"));test_idx=np.array(test_idx)\n",
    "\n",
    "    train_mask = sample_mask(train_idx, y.shape[0])\n",
    "    val_mask = sample_mask(val_idx, y.shape[0])\n",
    "    test_mask = sample_mask(test_idx, y.shape[0])\n",
    "    \n",
    "    y_train = np.zeros(y.shape)\n",
    "    y_val = np.zeros(y.shape)\n",
    "    y_test = np.zeros(y.shape)\n",
    "    y=y.values\n",
    "    y_train[train_mask, :] = y[train_mask, :]\n",
    "    y_val[val_mask, :] = y[val_mask, :]\n",
    "    y_test[test_mask, :] = y[test_mask, :]\n",
    "\n",
    "    # return selected_idx, selected_idx_2\n",
    "    print('y_train:{}, y_val:{}, y_test:{}, train_idx:{}, val_idx:{}, test_idx:{}'.format(y_train.shape,\n",
    "                                                                                          y_val.shape,\n",
    "                                                                                          y_test.shape,\n",
    "                                                                                          train_idx.shape,\n",
    "                                                                                          val_idx.shape,\n",
    "                                                                                          test_idx.shape))\n",
    "    truefeatures_list = [truefeatures, truefeatures]#, truefeatures, truefeatures]# ?? why copy three times? First for center node, 2 for each metapath.    \n",
    "    os.chdir(path_code)\n",
    "    return rownetworks, truefeatures_list, y_train, y_val, y_test, train_mask, val_mask, test_mask\n",
    "\n",
    "\n",
    "# use adj_list as fea_list, have a try~\n",
    "adj_list, fea_list, y_train, y_val, y_test, train_mask, val_mask, test_mask = load_data_dblp()\n",
    "if featype == 'adj':\n",
    "    fea_list = adj_list\n",
    "\n",
    "import scipy.sparse as sp\n",
    "\n",
    "nb_nodes = fea_list[0].shape[0]\n",
    "ft_size = fea_list[0].shape[1]\n",
    "nb_classes = y_train.shape[1]\n",
    "\n",
    "fea_list = [fea[np.newaxis] for fea in fea_list]\n",
    "adj_list = [adj[np.newaxis] for adj in adj_list]\n",
    "y_train = y_train[np.newaxis]\n",
    "y_val = y_val[np.newaxis]\n",
    "y_test = y_test[np.newaxis]\n",
    "train_mask = train_mask[np.newaxis]\n",
    "val_mask = val_mask[np.newaxis]\n",
    "test_mask = test_mask[np.newaxis]\n",
    "\n",
    "biases_list = [process.adj_to_bias(adj, [nb_nodes], nhood=1) for adj in adj_list]\n",
    "#biases_list = [process.adj_to_bias(adj) for adj in adj_list]\n",
    "#process.adj_to_bias(adj)\n",
    "print('build graph...')\n",
    "with tf.Graph().as_default():\n",
    "    with tf.name_scope('input'):\n",
    "        ftr_in_list = [tf.placeholder(dtype=tf.float32,\n",
    "                                      shape=(batch_size, nb_nodes, ft_size),\n",
    "                                      name='ftr_in_{}'.format(i))\n",
    "                       for i in range(len(fea_list))]\n",
    "        bias_in_list = [tf.placeholder(dtype=tf.float32,\n",
    "                                       shape=(batch_size, nb_nodes, nb_nodes),\n",
    "                                       name='bias_in_{}'.format(i))\n",
    "                        for i in range(len(biases_list))]\n",
    "        lbl_in = tf.placeholder(dtype=tf.int32, shape=(\n",
    "            batch_size, nb_nodes, nb_classes), name='lbl_in')\n",
    "        msk_in = tf.placeholder(dtype=tf.int32, shape=(batch_size, nb_nodes),\n",
    "                                name='msk_in')\n",
    "        attn_drop = tf.placeholder(dtype=tf.float32, shape=(), name='attn_drop')\n",
    "        ffd_drop = tf.placeholder(dtype=tf.float32, shape=(), name='ffd_drop')\n",
    "        is_train = tf.placeholder(dtype=tf.bool, shape=(), name='is_train')\n",
    "    # forward\n",
    "    logits, final_embedding, att_val = model.inference(ftr_in_list, nb_classes, nb_nodes, is_train,\n",
    "                                                       attn_drop, ffd_drop,\n",
    "                                                       bias_mat_list=bias_in_list,\n",
    "                                                       hid_units=hid_units, n_heads=n_heads,\n",
    "                                                       residual=residual, activation=nonlinearity)\n",
    "\n",
    "    # cal masked_loss\n",
    "    log_resh = tf.reshape(logits, [-1, nb_classes])\n",
    "    lab_resh = tf.reshape(lbl_in, [-1, nb_classes])\n",
    "    msk_resh = tf.reshape(msk_in, [-1])\n",
    "    loss = model.masked_softmax_cross_entropy(log_resh, lab_resh, msk_resh)\n",
    "    predicted=tf.argmax(log_resh, 1)\n",
    "    accuracy = model.masked_accuracy(log_resh, lab_resh, msk_resh)\n",
    "    # optimzie\n",
    "    train_op = model.training(loss, lr, l2_coef)\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    init_op = tf.group(tf.global_variables_initializer(),\n",
    "                       tf.local_variables_initializer())\n",
    "\n",
    "    vlss_mn = np.inf\n",
    "    vacc_mx = 0.0\n",
    "    curr_step = 0\n",
    "\n",
    "    with tf.Session(config=config) as sess:\n",
    "        sess.run(init_op)\n",
    "        train_loss_avg = 0\n",
    "        train_acc_avg = 0\n",
    "        val_loss_avg = 0\n",
    "        val_acc_avg = 0\n",
    "        val_pred_avg = 0\n",
    "        for epoch in range(nb_epochs):\n",
    "            tr_step = 0\n",
    "           \n",
    "            tr_size = fea_list[0].shape[0]\n",
    "            # ================   training    ============\n",
    "            while tr_step * batch_size < tr_size:\n",
    "\n",
    "                fd1 = {i: d[tr_step * batch_size:(tr_step + 1) * batch_size]\n",
    "                       for i, d in zip(ftr_in_list, fea_list)}\n",
    "                fd2 = {i: d[tr_step * batch_size:(tr_step + 1) * batch_size]\n",
    "                       for i, d in zip(bias_in_list, biases_list)}\n",
    "                fd3 = {lbl_in: y_train[tr_step * batch_size:(tr_step + 1) * batch_size],\n",
    "                       msk_in: train_mask[tr_step * batch_size:(tr_step + 1) * batch_size],\n",
    "                       is_train: True,\n",
    "                       attn_drop: 0,\n",
    "                       ffd_drop: 0}\n",
    "                fd = fd1\n",
    "                fd.update(fd2)\n",
    "                fd.update(fd3)\n",
    "                _, loss_value_tr, acc_tr, att_val_train = sess.run([train_op, loss, accuracy, att_val],\n",
    "                                                                   feed_dict=fd)\n",
    "                train_loss_avg += loss_value_tr\n",
    "                train_acc_avg += acc_tr\n",
    "                tr_step += 1\n",
    "\n",
    "            vl_step = 0\n",
    "            vl_size = fea_list[0].shape[0]\n",
    "\n",
    "            # import pdb; pdb.set_trace()\n",
    "            print('Epoch: {}, att_val: {}'.format(epoch, np.mean(att_val_train, axis=0)))\n",
    "            print('Training: loss = %.5f, acc = %.5f |' %\n",
    "                  (train_loss_avg / tr_step, train_acc_avg / tr_step))\n",
    "\n",
    "            curr_step += 1\n",
    "            if epoch==nb_epochs-1:\n",
    "                saver.save(sess, checkpt_file)\n",
    "\n",
    "            train_loss_avg = 0\n",
    "            train_acc_avg = 0\n",
    "            val_loss_avg = 0\n",
    "            val_acc_avg = 0\n",
    "        saver.restore(sess, checkpt_file)\n",
    "        print('load model from : {}'.format(checkpt_file))\n",
    "        ts_size = fea_list[0].shape[0]\n",
    "        ts_step = 0\n",
    "        ts_loss = 0.0\n",
    "        ts_acc = 0.0\n",
    "\n",
    "        while ts_step * batch_size < ts_size:\n",
    "            fd1 = {i: d[ts_step * batch_size:(ts_step + 1) * batch_size]\n",
    "                   for i, d in zip(ftr_in_list, fea_list)}\n",
    "            fd2 = {i: d[ts_step * batch_size:(ts_step + 1) * batch_size]\n",
    "                   for i, d in zip(bias_in_list, biases_list)}\n",
    "            fd3 = {lbl_in: y_test[ts_step * batch_size:(ts_step + 1) * batch_size],\n",
    "                   msk_in: test_mask[ts_step * batch_size:(ts_step + 1) * batch_size],\n",
    "            \n",
    "                   is_train: False,\n",
    "                   attn_drop: 0.0,\n",
    "                   ffd_drop: 0.0}\n",
    "        \n",
    "            fd = fd1\n",
    "            fd.update(fd2)\n",
    "            fd.update(fd3)\n",
    "            loss_value_ts, acc_ts, jhy_final_embedding = sess.run([loss, accuracy, final_embedding],\n",
    "                                                                  feed_dict=fd)\n",
    "            ts_loss += loss_value_ts\n",
    "            ts_acc += acc_ts\n",
    "            ts_step += 1\n",
    "\n",
    "        print('Test loss:', ts_loss / ts_step,\n",
    "              '; Test accuracy:', ts_acc / ts_step)\n",
    "        \n",
    "        print('start knn, kmean.....')\n",
    "        xx = np.expand_dims(jhy_final_embedding, axis=0)[test_mask]\n",
    "\n",
    "        from numpy import linalg as LA\n",
    "\n",
    "        yy = y_test[test_mask]\n",
    "\n",
    "        print('xx: {}, yy: {}'.format(xx.shape, yy.shape))\n",
    "        from jhyexp import my_KNN, my_Kmeans#, my_TSNE, my_Linear\n",
    "\n",
    "        my_KNN(xx, yy)\n",
    "        my_Kmeans(xx, yy)\n",
    "\n",
    "        sess.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "middle-therapist",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run previous block before running this one\n",
    "#get the testing result without submission data, i.e., the graph that this block used only contains comments data\n",
    "#the printed output is meaningless\n",
    "#predicted results are saved to .p file\n",
    "#prediction in this step is used in step 4, but not in step 5\n",
    "#if you have memory error, calculate \"biases_list\" variable for each school and save it locally, then jsut load during running. The code for this is commented out and you can use it directly\n",
    "\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "os.chdir(path_code)\n",
    "from gat import GAT, HeteGAT, HeteGAT_multi\n",
    "import process\n",
    "import importlib\n",
    "from layers import attn_head, SimpleAttLayer\n",
    "importlib.reload(process)\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import scipy.sparse\n",
    "from scipy.sparse import csc_matrix\n",
    "import scipy import sparse\n",
    "import scipy.io as sio\n",
    "import scipy.sparse as sp\n",
    "\n",
    "import csv \n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "dataset = 'acm'\n",
    "featype = 'fea'\n",
    "print('Dataset: ' + dataset)\n",
    "print('----- Opt. hyperparams -----')\n",
    "\n",
    "# jhy data\n",
    "import scipy.io as sio\n",
    "import scipy.sparse as sp\n",
    "\n",
    "\n",
    "def sample_mask(idx, l):\n",
    "    \"\"\"Create mask.\"\"\"\n",
    "    mask = np.zeros(l)\n",
    "    mask[idx] = 1\n",
    "    return np.array(mask, dtype=np.bool)\n",
    "\n",
    "schools = [\"notredame2019\",\"notredame2020\",\"uofm2019\",\"uofm2020\",\"columbia2019\",\"columbia2020\",\"dartmouth\",\"UCSD2019\",\"UCSD2020\",\"berkeley2019\",\"berkeley2020\",\"Harvard2019\",\"Harvard2020\",\"ucla2019\",\"ucla2020\"]\n",
    "schools_type = [\"notredamecomment2019\",\"notredamecomment2020\",\"uofmcomment2019\",\"uofmcomment2020\",\"columbiacomment2019\",\"columbiacomment2020\",\"dartmouthcomment\",\"UCSDcomment2019\",\"UCSDcomment2020\",\n",
    "           \"berkeleycomment2019\",\"berkeleycomment2020\",\"Harvardcomment2019\",\"Harvardcomment2020\",\"uclacomment2019\",\"uclacomment2020\"]\n",
    "\n",
    "def load_data_dblp(s,st,path=path_data):\n",
    "    t=\"comment\"\n",
    "    \n",
    "    os.chdir(path)\n",
    "    name=\"label_emt3_\"+s+\"_cm.csv\"\n",
    "    truelabels=pd.read_csv(name,skip_blank_lines=True,header=None)\n",
    "    name=\"feature_\"+st+\".csv\"\n",
    "    truefeatures=pd.read_csv(name,skip_blank_lines=True,header=None).values\n",
    "\n",
    "    N=truefeatures.shape[0]\n",
    "    name=\"CCsym_\"+s+\".npz\"\n",
    "    dat_cc=scipy.sparse.load_npz(name);dat_cc=dat_cc.toarray();\n",
    "    rownetworks = [(dat_cc)]\n",
    "\n",
    "    y=truelabels\n",
    "    name=\"train_index_\"+s+\"_cm.p\"\n",
    "    train_idx = pickle.load(open(name,\"rb\"));train_idx=np.array(train_idx)\n",
    "    name=\"to_be_labeled_index_\"+s+\"_cm.p\"\n",
    "    val_idx = pickle.load(open(name,\"rb\"));val_idx=np.array(val_idx)\n",
    "    name=\"test_index_\"+s+\"_cm.p\"\n",
    "    test_idx = pickle.load(open(name,\"rb\"));test_idx=np.array(test_idx)\n",
    "    print(len(test_idx))\n",
    "    #use this line to control testing set\n",
    "    test_idx=np.array(range(N))\n",
    "    \n",
    "    train_mask = sample_mask(train_idx, y.shape[0])\n",
    "    val_mask = sample_mask(val_idx, y.shape[0])\n",
    "    test_mask = sample_mask(test_idx, y.shape[0])# ?? why copy three times? First for center node, 2 for each metapath.\n",
    "    \n",
    "    y_train = np.zeros(y.shape)\n",
    "    y_val = np.zeros(y.shape)\n",
    "    y_test = np.zeros(y.shape)\n",
    "    y=y.values\n",
    "    y_train[train_mask, :] = y[train_mask, :]\n",
    "    y_val[val_mask, :] = y[val_mask, :]\n",
    "    y_test[test_mask, :] = y[test_mask, :]\n",
    "\n",
    "    # return selected_idx, selected_idx_2\n",
    "    print('y_train:{}, y_val:{}, y_test:{}, train_idx:{}, val_idx:{}, test_idx:{}'.format(y_train.shape,\n",
    "                                                                                          y_val.shape,\n",
    "                                                                                          y_test.shape,\n",
    "                                                                                          train_idx.shape,\n",
    "                                                                                          val_idx.shape,\n",
    "                                                                                          test_idx.shape))\n",
    "    truefeatures_list = [truefeatures, truefeatures]#, truefeatures, truefeatures]# ?? why copy three times? First for center node, 2 for each metapath.    \n",
    "    os.chdir(path_code)\n",
    "    return rownetworks, truefeatures_list, y_train, y_val, y_test, train_mask, val_mask, test_mask\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(schools)):\n",
    "    adj_list, fea_list, y_train, y_val, y_test, train_mask, val_mask, test_mask = load_data_dblp(s=schools[i],st=schools_type[i])\n",
    "    \n",
    "\n",
    "    if featype == 'adj':\n",
    "        fea_list = adj_list\n",
    "\n",
    "\n",
    "\n",
    "    import scipy.sparse as sp\n",
    "\n",
    "    nb_nodes = fea_list[0].shape[0]\n",
    "    ft_size = fea_list[0].shape[1]\n",
    "    nb_classes = y_train.shape[1]\n",
    "    \n",
    "    fea_list = [fea[np.newaxis] for fea in fea_list]\n",
    "    adj_list = [adj[np.newaxis] for adj in adj_list]\n",
    "    y_train = y_train[np.newaxis]\n",
    "    y_val = y_val[np.newaxis]\n",
    "    y_test = y_test[np.newaxis]\n",
    "    train_mask = train_mask[np.newaxis]\n",
    "    val_mask = val_mask[np.newaxis]\n",
    "    test_mask = test_mask[np.newaxis]\n",
    "    print(\"s\")\n",
    "    \n",
    "    biases_list = [process.adj_to_bias(adj, [nb_nodes], nhood=1) for adj in adj_list]\n",
    "    name=schools[i]+'adj.p'\n",
    "    os.chdir(path_data)\n",
    "    pickle.dump(biases_list,open(name,\"wb\"))\n",
    "    #biases_list=pickle.load(open(name,\"rb\"))\n",
    "    \n",
    "    \n",
    "    print('build graph...')\n",
    "    with tf.Graph().as_default():\n",
    "        with tf.name_scope('input'):\n",
    "            ftr_in_list = [tf.placeholder(dtype=tf.float32,\n",
    "                                          shape=(batch_size, nb_nodes, ft_size),\n",
    "                                          name='ftr_in_{}'.format(i))\n",
    "                           for i in range(len(fea_list))]\n",
    "            bias_in_list = [tf.placeholder(dtype=tf.float32,\n",
    "                                           shape=(batch_size, nb_nodes, nb_nodes),\n",
    "                                           name='bias_in_{}'.format(i))\n",
    "                            for i in range(len(biases_list))]\n",
    "            lbl_in = tf.placeholder(dtype=tf.int32, shape=(\n",
    "                batch_size, nb_nodes, nb_classes), name='lbl_in')\n",
    "            msk_in = tf.placeholder(dtype=tf.int32, shape=(batch_size, nb_nodes),\n",
    "                                    name='msk_in')\n",
    "            attn_drop = tf.placeholder(dtype=tf.float32, shape=(), name='attn_drop')\n",
    "            ffd_drop = tf.placeholder(dtype=tf.float32, shape=(), name='ffd_drop')\n",
    "            is_train = tf.placeholder(dtype=tf.bool, shape=(), name='is_train')\n",
    "        # forward\n",
    "        logits, final_embedding, att_val = model.inference(ftr_in_list, nb_classes, nb_nodes, is_train,attn_drop, ffd_drop,bias_mat_list=bias_in_list,hid_units=hid_units, n_heads=n_heads,residual=residual, activation=nonlinearity)\n",
    "    \n",
    "        # cal masked_loss\n",
    "        log_resh = tf.reshape(logits, [-1, nb_classes])\n",
    "        lab_resh = tf.reshape(lbl_in, [-1, nb_classes])\n",
    "        msk_resh = tf.reshape(msk_in, [-1])\n",
    "        loss = model.masked_softmax_cross_entropy(log_resh, lab_resh, msk_resh)\n",
    "        accuracy = model.masked_accuracy(log_resh, lab_resh, msk_resh)\n",
    "        # optimzie\n",
    "        train_op = model.training(loss, lr, l2_coef)\n",
    "        vlss_mn = np.inf\n",
    "        vacc_mx = 0.0\n",
    "        curr_step = 0\n",
    "        saver = tf.train.Saver()\n",
    "        init_op = tf.group(tf.global_variables_initializer(),\n",
    "                           tf.local_variables_initializer())\n",
    "        with tf.Session(config=config) as sess:\n",
    "            #sess.run(init_op)\n",
    "\n",
    "            train_loss_avg = 0\n",
    "            train_acc_avg = 0\n",
    "            val_loss_avg = 0\n",
    "            val_acc_avg = 0\n",
    "            os.chdir(path_code)\n",
    "            saver.restore(sess, path_result+\"acm_allMP_multi_fea_.ckpt\")\n",
    "            print(\"Model restored.\")\n",
    "            \n",
    "            ts_size = fea_list[0].shape[0]\n",
    "            print(fea_list[0].shape)\n",
    "            ts_step = 0\n",
    "            ts_loss = 0.0\n",
    "            ts_acc = 0.0\n",
    "            print(\"test start\")\n",
    "            while ts_step * batch_size < ts_size:\n",
    "                print(ts_step)\n",
    "                print(batch_size)\n",
    "                print(ts_size)\n",
    "                # fd1 = {ftr_in: features[ts_step * batch_size:(ts_step + 1) * batch_size]}\n",
    "                fd1 = {i: d[ts_step * batch_size:(ts_step + 1) * batch_size]\n",
    "                       for i, d in zip(ftr_in_list, fea_list)}\n",
    "                fd2 = {i: d[ts_step * batch_size:(ts_step + 1) * batch_size]\n",
    "                       for i, d in zip(bias_in_list, biases_list)}\n",
    "                fd3 = {lbl_in: y_test[ts_step * batch_size:(ts_step + 1) * batch_size],\n",
    "                       msk_in: test_mask[ts_step * batch_size:(ts_step + 1) * batch_size],\n",
    "                \n",
    "                       is_train: False,\n",
    "                       attn_drop: 0.0,\n",
    "                       ffd_drop: 0.0}\n",
    "            \n",
    "                fd = fd1\n",
    "                fd.update(fd2)\n",
    "                fd.update(fd3)\n",
    "                \n",
    "                loss_value_ts, acc_ts, jhy_final_embedding, predicted = sess.run([loss, accuracy, final_embedding, log_resh],\n",
    "                                                                      feed_dict=fd)\n",
    "                import pickle\n",
    "                name=schools[i]+'.p'\n",
    "                os.chdir(path_result)\n",
    "                pickle.dump(predicted,open(name,\"wb\"))\n",
    "                ts_loss += loss_value_ts\n",
    "                ts_acc += acc_ts\n",
    "                ts_step += 1\n",
    "\n",
    "            print('Test loss:', ts_loss / ts_step,\n",
    "                  '; Test accuracy:', ts_acc / ts_step)\n",
    "    \n",
    "            \n",
    "            sess.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opponent-november",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run first block before running this one\n",
    "#get the testing result WITH submission data, i.e., the graph that this block contains comments and submission\n",
    "#the printed output is meaningless\n",
    "#predicted results are saved to .p file\n",
    "#prediction in this step is used in step 5, but not in step 4\n",
    "#if you have memory error, calculate \"biases_list\" variable for each school and save it locally. The code for this is commented out and you can use it directly\n",
    "\n",
    "#The feature, index, adjacency, label data are all different from the one used in previou block, contact the author to get these two sets of data\n",
    "\n",
    "path_result='/content/drive/MyDrive/Colab Notebooks/results/'\n",
    "path_data=\"/content/drive/MyDrive/Colab Notebooks/data/\"\n",
    "path_code=\"/content/drive/MyDrive/Colab Notebooks/\"\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "os.chdir(path_code)\n",
    "from gat import GAT, HeteGAT, HeteGAT_multi\n",
    "import process\n",
    "import importlib\n",
    "from layers import attn_head, SimpleAttLayer\n",
    "importlib.reload(process)\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import scipy.sparse\n",
    "from scipy.sparse import csc_matrix\n",
    "import scipy import sparse\n",
    "import scipy.io as sio\n",
    "import scipy.sparse as sp\n",
    "\n",
    "import csv \n",
    "    \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2,3\"\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "dataset = 'acm'\n",
    "featype = 'fea'\n",
    "# training params\n",
    "\n",
    "checkpt_file=path_result+'acm_allMP_multi_fea_.ckpt'\n",
    "\n",
    "\n",
    "print('Dataset: ' + dataset)\n",
    "print('----- Opt. hyperparams -----')\n",
    "\n",
    "\n",
    "\n",
    "def sample_mask(idx, l):\n",
    "    \"\"\"Create mask.\"\"\"\n",
    "    mask = np.zeros(l)\n",
    "    mask[idx] = 1\n",
    "    return np.array(mask, dtype=np.bool)\n",
    "\n",
    "#schools = [\"berkeley2020\"]\n",
    "schools =[\"notredame2019\",\"notredame2020\",\"uofm2019\",\"uofm2020\",\"columbia2019\",\"columbia2020\",\"dartmouth\",\"UCSD2019\",\"UCSD2020\",\"berkeley2019\",\"berkeley2020\",\"Harvard2019\",\"Harvard2020\",\"ucla2019\",\"ucla2020\"]\n",
    "schools_type = [\"notredamecomment2019\",\"notredamecomment2020\",\"uofmcomment2019\",\"uofmcomment2020\",\"columbiacomment2019\",\"columbiacomment2020\",\"dartmouthcomment\",\"UCSDcomment2019\",\"UCSDcomment2020\",\n",
    "           \"berkeleycomment2019\",\"berkeleycomment2020\",\"Harvardcomment2019\",\"Harvardcomment2020\",\"uclacomment2019\",\"uclacomment2020\"]\n",
    "\n",
    "\n",
    "#path_data='C:\\\\data\\\\HAN-modified\\\\tranfer learning\\\\'\n",
    "def load_data_dblp(s,path=path_data):\n",
    "    t=\"comment\"\n",
    "    \n",
    "    os.chdir(path)\n",
    "    name=\"label_emt3_\"+s+\".csv\"\n",
    "    truelabels=pd.read_csv(name,skip_blank_lines=True,header=None)\n",
    "    name=\"feature_\"+s+\".csv\"\n",
    "    truefeatures=pd.read_csv(name,skip_blank_lines=True,header=None).values\n",
    "    N=truefeatures.shape[0]\n",
    "    name=\"CCsym_\"+s+\".npz\"\n",
    "    dat_cc=scipy.sparse.load_npz(name);\n",
    "    dat_cc=dat_cc.toarray();\n",
    "    rownetworks = [(dat_cc)]\n",
    "\n",
    "    y=truelabels\n",
    "    name=\"train_index_\"+s+\"_cm.p\"\n",
    "    train_idx = pickle.load(open(name,\"rb\"));train_idx=np.array(train_idx)\n",
    "    name=\"to_be_labeled_index_\"+s+\"_cm.p\"\n",
    "    val_idx = pickle.load(open(name,\"rb\"));val_idx=np.array(val_idx)\n",
    "    name=\"test_index_\"+s+\"_cm.p\"\n",
    "    test_idx = pickle.load(open(name,\"rb\"));test_idx=np.array(test_idx)\n",
    "    test_idx=np.array(range(N))\n",
    "    print(N)\n",
    "    \n",
    "    train_mask = sample_mask(train_idx, y.shape[0])\n",
    "    val_mask = sample_mask(val_idx, y.shape[0])\n",
    "    test_mask = sample_mask(test_idx, y.shape[0])# ?? why copy three times? First for center node, 2 for each metapath.\n",
    "    \n",
    "    y_train = np.zeros(y.shape)\n",
    "    y_val = np.zeros(y.shape)\n",
    "    y_test = np.zeros(y.shape)\n",
    "    y=y.values\n",
    "    y_train[train_mask, :] = y[train_mask, :]\n",
    "    y_val[val_mask, :] = y[val_mask, :]\n",
    "    y_test[test_mask, :] = y[test_mask, :]\n",
    "\n",
    "    # return selected_idx, selected_idx_2\n",
    "    print('y_train:{}, y_val:{}, y_test:{}, train_idx:{}, val_idx:{}, test_idx:{}'.format(y_train.shape,\n",
    "                                                                                          y_val.shape,\n",
    "                                                                                          y_test.shape,\n",
    "                                                                                          train_idx.shape,\n",
    "                                                                                          val_idx.shape,\n",
    "                                                                                          test_idx.shape))\n",
    "    truefeatures_list = [truefeatures, truefeatures]#, truefeatures, truefeatures]# ?? why copy three times? First for center node, 2 for each metapath.    \n",
    "    os.chdir(path_code)\n",
    "    return rownetworks, truefeatures_list, y_train, y_val, y_test, train_mask, val_mask, test_mask\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(schools)):\n",
    "    adj_list, fea_list, y_train, y_val, y_test, train_mask, val_mask, test_mask = load_data_dblp(s=schools[i])\n",
    "    if featype == 'adj':\n",
    "        fea_list = adj_list\n",
    "\n",
    "    nb_nodes = fea_list[0].shape[0]\n",
    "    ft_size = fea_list[0].shape[1]\n",
    "    nb_classes = y_train.shape[1]\n",
    "\n",
    "    fea_list = [fea[np.newaxis] for fea in fea_list]\n",
    "    adj_list = [adj[np.newaxis] for adj in adj_list]\n",
    "    y_train = y_train[np.newaxis]\n",
    "    y_val = y_val[np.newaxis]\n",
    "    y_test = y_test[np.newaxis]\n",
    "    train_mask = train_mask[np.newaxis]\n",
    "    val_mask = val_mask[np.newaxis]\n",
    "    test_mask = test_mask[np.newaxis]\n",
    "    print(\"s\")\n",
    "    \n",
    "    biases_list = [process.adj_to_bias(adj, [nb_nodes], nhood=1) for adj in adj_list]\n",
    "    import pickle\n",
    "    name=schools[i]+'adj.p'\n",
    "    os.chdir(path_data)\n",
    "    pickle.dump(biases_list,open(name,\"wb\"))\n",
    "    #biases_list=pickle.load(open(name,\"rb\"))\n",
    "    \n",
    "    print('build graph...')\n",
    "    with tf.Graph().as_default():\n",
    "        with tf.name_scope('input'):\n",
    "            ftr_in_list = [tf.placeholder(dtype=tf.float32,\n",
    "                                          shape=(batch_size, nb_nodes, ft_size),\n",
    "                                          name='ftr_in_{}'.format(i))\n",
    "                           for i in range(len(fea_list))]\n",
    "            bias_in_list = [tf.placeholder(dtype=tf.float32,\n",
    "                                           shape=(batch_size, nb_nodes, nb_nodes),\n",
    "                                           name='bias_in_{}'.format(i))\n",
    "                            for i in range(len(biases_list))]\n",
    "            lbl_in = tf.placeholder(dtype=tf.int32, shape=(\n",
    "                batch_size, nb_nodes, nb_classes), name='lbl_in')\n",
    "            msk_in = tf.placeholder(dtype=tf.int32, shape=(batch_size, nb_nodes),\n",
    "                                    name='msk_in')\n",
    "            attn_drop = tf.placeholder(dtype=tf.float32, shape=(), name='attn_drop')\n",
    "            ffd_drop = tf.placeholder(dtype=tf.float32, shape=(), name='ffd_drop')\n",
    "            is_train = tf.placeholder(dtype=tf.bool, shape=(), name='is_train')\n",
    "        # forward\n",
    "        logits, final_embedding, att_val = model.inference(ftr_in_list, nb_classes, nb_nodes, is_train,attn_drop, ffd_drop,bias_mat_list=bias_in_list,hid_units=hid_units, n_heads=n_heads,residual=residual, activation=nonlinearity)\n",
    "    \n",
    "        # cal masked_loss\n",
    "        log_resh = tf.reshape(logits, [-1, nb_classes])\n",
    "        lab_resh = tf.reshape(lbl_in, [-1, nb_classes])\n",
    "        msk_resh = tf.reshape(msk_in, [-1])\n",
    "        loss = model.masked_softmax_cross_entropy(log_resh, lab_resh, msk_resh)\n",
    "        accuracy = model.masked_accuracy(log_resh, lab_resh, msk_resh)\n",
    "        # optimzie\n",
    "        train_op = model.training(loss, lr, l2_coef)\n",
    "        vlss_mn = np.inf\n",
    "        vacc_mx = 0.0\n",
    "        curr_step = 0\n",
    "        saver = tf.train.Saver()\n",
    "        init_op = tf.group(tf.global_variables_initializer(),\n",
    "                           tf.local_variables_initializer())\n",
    "        with tf.Session(config=config) as sess:\n",
    "            #sess.run(init_op)\n",
    "\n",
    "            train_loss_avg = 0\n",
    "            train_acc_avg = 0\n",
    "            val_loss_avg = 0\n",
    "            val_acc_avg = 0\n",
    "            os.chdir(path_code)\n",
    "            saver.restore(sess, path_result+\"acm_allMP_multi_fea_.ckpt\")\n",
    "            print(\"Model restored.\")\n",
    "            \n",
    "            ts_size = fea_list[0].shape[0]\n",
    "            print(fea_list[0].shape)\n",
    "            ts_step = 0\n",
    "            ts_loss = 0.0\n",
    "            ts_acc = 0.0\n",
    "            print(\"test start\")\n",
    "            while ts_step * batch_size < ts_size:\n",
    "                print(ts_step)\n",
    "                print(batch_size)\n",
    "                print(ts_size)\n",
    "                # fd1 = {ftr_in: features[ts_step * batch_size:(ts_step + 1) * batch_size]}\n",
    "                fd1 = {i: d[ts_step * batch_size:(ts_step + 1) * batch_size]\n",
    "                       for i, d in zip(ftr_in_list, fea_list)}\n",
    "                fd2 = {i: d[ts_step * batch_size:(ts_step + 1) * batch_size]\n",
    "                       for i, d in zip(bias_in_list, biases_list)}\n",
    "                fd3 = {lbl_in: y_test[ts_step * batch_size:(ts_step + 1) * batch_size],\n",
    "                       msk_in: test_mask[ts_step * batch_size:(ts_step + 1) * batch_size],\n",
    "                \n",
    "                       is_train: False,\n",
    "                       attn_drop: 0.0,\n",
    "                       ffd_drop: 0.0}\n",
    "                import psutil\n",
    "              \n",
    "                print(psutil.virtual_memory().available * 100 / psutil.virtual_memory().total)\n",
    "                fd = fd1\n",
    "                fd.update(fd2)\n",
    "                fd.update(fd3)\n",
    "                print('fini')\n",
    "                del bias_in_list\n",
    "                del biases_list\n",
    "                del ftr_in_list\n",
    "                del fea_list\n",
    "                print('dele finished')\n",
    "                print(psutil.virtual_memory().available * 100 / psutil.virtual_memory().total)\n",
    "                loss_value_ts, acc_ts, jhy_final_embedding, predicted = sess.run([loss, accuracy, final_embedding, log_resh],\n",
    "                                                                      feed_dict=fd)\n",
    "                import pickle\n",
    "                name=schools[i]+'.p'\n",
    "                os.chdir(path_result)\n",
    "                pickle.dump(predicted,open(name,\"wb\"))\n",
    "                ts_loss += loss_value_ts\n",
    "                ts_acc += acc_ts\n",
    "                ts_step += 1\n",
    "\n",
    "            print('Test loss:', ts_loss / ts_step,\n",
    "                  '; Test accuracy:', ts_acc / ts_step)\n",
    "    \n",
    "            \n",
    "            sess.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
